---
title: MongoDB Change Stream Limitations
description: Learn about the limitations and constraints of MongoDB Change Streams, including deployment requirements, resume capabilities, and event handling restrictions.

---

# MongoDB Change Stream Limitations

When working with MongoDB Change Streams, it's important to understand their limitations to design robust and efficient applications. While Change Streams offer powerful capabilities for real-time data monitoring, there are constraints you should be aware of.

## Introduction

MongoDB Change Streams provide a mechanism to watch collections, databases, or entire deployments for data changes. However, like any technology, they come with certain limitations that might affect your implementation choices. This guide explores these limitations to help you make informed decisions when designing applications that rely on Change Streams.

## Deployment Requirements

### Replica Set or Sharded Cluster Requirement

One of the most fundamental limitations of Change Streams is that they require a replica set or sharded cluster deployment.

```mermaid
flowchart TD
    A[MongoDB Deployment] --> B{Supports Change Streams?}
    B -->|Yes| C[Replica Set]
    B -->|Yes| D[Sharded Cluster]
    B -->|No| E[Standalone Server]
```

Change Streams depend on the oplog (operations log) that is only available in replica set configurations. If you're using a standalone MongoDB server, you won't be able to use Change Streams.

```javascript
// This will fail on a standalone MongoDB server
try {
  const changeStream = db.collection('users').watch();
  console.log('Change stream established');
} catch (error) {
  console.error('Failed to establish change stream:', error.message);
  // Potential output: MongoError: $changeStream requires replication to be enabled
}
```

### Storage Engine Requirements

Change Streams are only supported by the WiredTiger storage engine, which has been the default since MongoDB 3.2. If you're using a different storage engine, you won't be able to use Change Streams.

## Event Types and Filtering Limitations

### Limited Event Types

Change Streams capture the following operations:
- Insert
- Update
- Replace
- Delete
- Drop
- Rename
- Invalidate

However, they do not capture read operations or administrative commands that don't modify data directly.

### Pipeline Stage Limitations

While you can use aggregation pipeline stages with Change Streams, not all stages are supported:

```javascript
// Supported pipeline stage example
const pipeline = [
  { $match: { 'operationType': { $in: ['insert', 'delete'] } } },
  { $project: { '_id': 1, 'fullDocument': 1, 'ns': 1, 'documentKey': 1 } }
];

// Creating a change stream with the pipeline
const changeStream = collection.watch(pipeline);
```

Unsupported stages include:
- `$collStats`
- `$facet`
- `$geoNear`
- `$graphLookup`
- `$lookup`
- `$merge`
- `$out`
- `$unionWith`

## Resume Capabilities and Retention Period

### Oplog Size Impact

Change Streams rely on the oplog for resumability. The oplog has a finite size, and once operations roll off due to the oplog size limit, you cannot resume a change stream from a token that references those operations.

```javascript
// Example demonstrating resuming a change stream
async function handleChangeStreamWithResume(collection) {
  let resumeToken = null;
  
  try {
    const changeStream = collection.watch();
    
    changeStream.on('change', (change) => {
      console.log('Change detected:', change);
      // Store the resumeToken for potential resume after disconnection
      resumeToken = change._id;
    });
    
    // Simulate a disconnection after 5 seconds
    setTimeout(() => {
      console.log('Simulating disconnection...');
      changeStream.close();
      
      // Attempt to resume the change stream
      resumeWithToken(collection, resumeToken);
    }, 5000);
    
  } catch (error) {
    console.error('Error in change stream:', error);
  }
}

function resumeWithToken(collection, token) {
  if (!token) {
    console.log('No resume token available');
    return;
  }
  
  console.log('Resuming change stream from token:', token);
  try {
    const newChangeStream = collection.watch([], { resumeAfter: token });
    newChangeStream.on('change', (change) => {
      console.log('Change after resume:', change);
    });
    
    // Note: If the token references operations that are no longer in the oplog
    // this resume attempt will fail with an error like:
    // "resume token no longer in oplog"
  } catch (error) {
    console.error('Failed to resume change stream:', error);
  }
}
```

### Default Retention Period

The default retention period for the oplog is typically 24 hours, but it can vary based on server activity and configuration. To mitigate this limitation, consider:

1. Increasing the oplog size (requires planning for storage impact)
2. Implementing robust error handling when resuming streams
3. Having fallback mechanisms to rebuild state if resuming fails

## Performance Considerations

### Resource Consumption

Change Streams consume resources on both the server and client sides:

1. **Server**: Each change stream operation requires server resources to track and deliver changes
2. **Client**: Maintaining open cursors and processing events requires client-side resources

For high-volume collections with many watchers, this can impact overall system performance.

### Connection Overhead

Each open change stream maintains a connection to the MongoDB server. In applications with many change streams, this can lead to connection pool exhaustion.

```javascript
// Anti-pattern: Creating too many change streams
async function monitorManyCollections(db, collectionNames) {
  // This approach could lead to too many open connections
  const streams = [];
  
  for (const name of collectionNames) {
    const collection = db.collection(name);
    const changeStream = collection.watch();
    
    changeStream.on('change', (change) => {
      console.log(`Change in ${name}:`, change);
    });
    
    streams.push(changeStream);
  }
  
  return streams;
}
```

A better approach is to watch at the database or deployment level and filter events:

```javascript
// Better pattern: Single change stream with filtering
async function monitorManyCollectionsEfficiently(db, collectionNames) {
  // Create a pipeline that filters for specific collections
  const pipeline = [
    {
      $match: {
        'ns.coll': { $in: collectionNames }
      }
    }
  ];
  
  // Watch at the database level with filtering
  const changeStream = db.watch(pipeline);
  
  changeStream.on('change', (change) => {
    console.log(`Change in ${change.ns.coll}:`, change);
  });
  
  return changeStream;
}
```

## Cross-Shard Limitations in Sharded Clusters

In sharded clusters, certain operations may not appear in change streams in a consistent order across shards. This is especially important for applications that require strict ordering of events.

### Shard Migration Impacts

During chunk migration between shards, there might be temporary inconsistencies in change stream event ordering.

### Multi-document Transaction Limitations

While multi-document transactions are supported in change streams, there are some caveats:

1. Change events for individual operations within a transaction are only visible after the transaction commits
2. The events will have the same clusterTime but will maintain their operation order
3. The events won't include a txnNumber field unless the transactions feature compatibility version (fcv) is "4.2" or greater

```javascript
// Example showing transaction events in a change stream
async function monitorTransactionEvents(collection) {
  const changeStream = collection.watch({ fullDocument: 'updateLookup' });
  
  changeStream.on('change', (change) => {
    if (change.operationType === 'insert' || change.operationType === 'update') {
      console.log('Operation type:', change.operationType);
      console.log('Document after change:', change.fullDocument);
      
      // For MongoDB 4.2+, you might see transaction information
      if (change.txnNumber) {
        console.log('Part of transaction:', change.txnNumber);
      }
    }
  });
  
  return changeStream;
}
```

## Security and Authentication Limitations

### Access Control Requirements

Users must have the appropriate privileges to create and use change streams:

- For collection-level streams: `find` on the specific collection
- For database-level streams: `find` on all collections in the database
- For deployment-level streams: `find` on all collections in all databases

```javascript
// Example of required privilege for change streams
// In MongoDB shell, you would create a user like this:
/*
db.createUser({
  user: "changeStreamUser",
  pwd: "password",
  roles: [
    { role: "read", db: "myDatabase" }
  ]
})
*/

// In your application, you would connect with these credentials:
const mongoClient = new MongoClient('mongodb://changeStreamUser:password@localhost:27017/myDatabase');
```

### TLS/SSL Considerations

For secure deployments, ensure that your change stream connections use TLS/SSL, especially in production environments.

## Real-World Example: E-commerce Inventory System

Consider an e-commerce platform using Change Streams to update inventory levels in real-time. Let's examine how these limitations might affect implementation:

```javascript
const { MongoClient } = require('mongodb');

async function monitorInventoryChanges() {
  let client;
  try {
    // Connect to MongoDB replica set (required for Change Streams)
    client = new MongoClient('mongodb://localhost:27017,localhost:27018,localhost:27019/ecommerce?replicaSet=rs0');
    await client.connect();
    
    const db = client.db('ecommerce');
    const inventoryCollection = db.collection('inventory');
    
    // Create a pipeline to filter only specific inventory changes
    const pipeline = [
      {
        $match: {
          $or: [
            { 'operationType': 'update', 'updateDescription.updatedFields.quantity': { $exists: true } },
            { 'operationType': 'insert' },
            { 'operationType': 'delete' }
          ]
        }
      },
      {
        $project: {
          '_id': 1,
          'operationType': 1,
          'fullDocument': 1,
          'documentKey': 1,
          'updateDescription': 1
        }
      }
    ];
    
    const changeStream = inventoryCollection.watch(pipeline, { fullDocument: 'updateLookup' });
    
    let resumeToken = null;
    
    changeStream.on('change', (change) => {
      // Store the latest resume token
      resumeToken = change._id;
      
      switch (change.operationType) {
        case 'insert':
          console.log(`New product added: ${change.fullDocument.name}, Quantity: ${change.fullDocument.quantity}`);
          // Update UI, notify warehouse, etc.
          break;
        case 'update':
          console.log(`Inventory updated for product: ${change.fullDocument.name}`);
          console.log(`New quantity: ${change.fullDocument.quantity}`);
          
          // Check if quantity is low and trigger alerts
          if (change.fullDocument.quantity < change.fullDocument.reorderThreshold) {
            console.log(`LOW STOCK ALERT: ${change.fullDocument.name} is below threshold!`);
            // Send notification to purchasing department
          }
          break;
        case 'delete':
          console.log(`Product removed from inventory: ${change.documentKey._id}`);
          // Update catalog, notify teams
          break;
      }
    });
    
    changeStream.on('error', async (error) => {
      console.error('Change stream error:', error);
      
      // Try to resume the change stream if possible
      if (resumeToken) {
        try {
          console.log('Attempting to resume change stream...');
          changeStream = inventoryCollection.watch(pipeline, { 
            resumeAfter: resumeToken,
            fullDocument: 'updateLookup'
          });
        } catch (resumeError) {
          console.error('Failed to resume change stream:', resumeError);
          // Implement fallback mechanism - perhaps a full resync of inventory data
        }
      }
    });
    
    console.log('Inventory monitoring started');
    
    // Keep the application running
    process.on('SIGINT', async () => {
      console.log('Closing change stream and connection...');
      await changeStream.close();
      await client.close();
      process.exit();
    });
    
  } catch (error) {
    console.error('Failed to set up inventory monitoring:', error);
    if (client) await client.close();
  }
}

monitorInventoryChanges().catch(console.error);
```

### Addressing Limitations in the E-commerce Example

1. **Deployment Requirement**: We connect to a replica set explicitly
2. **Resume Capability**: We store the resume token and attempt to resume on error
3. **Error Handling**: We implement proper error handling for the change stream
4. **Resource Management**: We optimize by using a targeted pipeline to filter only relevant changes
5. **Graceful Shutdown**: We properly close the change stream and connection when the application terminates

## Summary

MongoDB Change Streams offer powerful real-time monitoring capabilities but come with important limitations to consider:

1. **Deployment Requirements**:
   - Requires a replica set or sharded cluster
   - Not available on standalone servers
   - Only supported with WiredTiger storage engine

2. **Event and Filtering Limitations**:
   - Limited to specific operation types
   - Not all aggregation pipeline stages are supported

3. **Resume Capabilities**:
   - Dependent on oplog size and retention
   - Resume tokens may expire if operations roll off the oplog

4. **Performance Considerations**:
   - Resource consumption on both server and client
   - Connection overhead for multiple change streams

5. **Sharded Cluster Limitations**:
   - Event ordering may be inconsistent across shards
   - Special considerations for chunk migrations and transactions

By understanding these limitations and implementing appropriate strategies to address them, you can build robust applications that leverage the real-time capabilities of MongoDB Change Streams effectively.

## Additional Resources

- [MongoDB Documentation: Change Streams](https://docs.mongodb.com/manual/changeStreams/)
- [MongoDB Documentation: Change Events](https://docs.mongodb.com/manual/reference/change-events/)
- [MongoDB Blog: Using Change Streams in MongoDB](https://www.mongodb.com/blog/post/an-introduction-to-change-streams)

## Exercises

1. **Resume Token Handling**: Implement a change stream with robust resume token persistence to a file or database
2. **Error Simulation**: Create a test that simulates a change stream disconnection and practice resuming
3. **Resource Optimization**: Convert multiple collection-level change streams to a single database-level stream with filtering
4. **Monitoring**: Add monitoring to track change stream performance and resource usage in your application
5. **Failover Testing**: Test how your change stream application behaves during a replica set primary failover