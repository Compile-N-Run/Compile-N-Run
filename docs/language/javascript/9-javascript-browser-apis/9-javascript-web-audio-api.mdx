---
title: JavaScript Web Audio API
description: Learn how to create, manipulate, and process audio in web applications using JavaScript's Web Audio API

---

# JavaScript Web Audio API

## Introduction

The Web Audio API provides a powerful system for controlling audio on the web, allowing developers to choose audio sources, add effects, create audio visualizations, apply spatial effects, and more. Unlike the simple HTML5 `<audio>` element, the Web Audio API offers precise control over audio, enabling everything from simple sound playback to complex audio synthesis and processing.

In this tutorial, we'll explore the core concepts of the Web Audio API and learn how to use it to create interactive audio experiences in your web applications.

## Prerequisites

Before diving into the Web Audio API, you should have:
- Basic understanding of JavaScript
- Familiarity with HTML and DOM manipulation

## Understanding the Audio Context

At the heart of the Web Audio API is the `AudioContext` object. This represents an audio processing graph built from audio nodes connected together, handling the creation of audio sources, processing, and destination output.

Let's create our first audio context:

```javascript
// Create an AudioContext
let audioContext;

// Browser compatibility handling
function initAudioContext() {
  try {
    // The newer standard implementation
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    console.log('Audio context created successfully');
  } catch(e) {
    console.error('Web Audio API is not supported in this browser', e);
  }
}

// Initialize on user interaction (needed for browsers that require user gesture)
document.querySelector('button').addEventListener('click', initAudioContext);
```

:::note
Many browsers now require a user gesture (like a click) before allowing audio playback, which is why we initiate the audio context on button click.
:::

## Building an Audio Graph

The Web Audio API works by connecting audio nodes together to form an audio routing graph. A typical graph flows from one or more sources through zero or more nodes to a destination.

Here's a visualization of a simple audio graph:

```
[Source] → [Processing Node(s)] → [Destination]
```

Common types of audio nodes include:
- **Source nodes**: Where audio originates (e.g., oscillators, audio files)
- **Processing nodes**: Modify audio (e.g., gain, filters, effects)
- **Destination node**: Where audio ultimately outputs (usually speakers)

## Creating and Playing Sounds

### Playing a Simple Tone (Oscillator)

Let's create a simple oscillator that generates a tone:

```javascript
function playTone(frequency = 440, type = 'sine', duration = 1) {
  // Create oscillator node
  const oscillator = audioContext.createOscillator();
  
  // Configure oscillator
  oscillator.type = type; // 'sine', 'square', 'sawtooth', 'triangle'
  oscillator.frequency.setValueAtTime(frequency, audioContext.currentTime);
  
  // Create gain node for volume control and to avoid clicks
  const gainNode = audioContext.createGain();
  gainNode.gain.setValueAtTime(1, audioContext.currentTime);
  gainNode.gain.exponentialRampToValueAtTime(0.001, audioContext.currentTime + duration);
  
  // Connect nodes
  oscillator.connect(gainNode);
  gainNode.connect(audioContext.destination);
  
  // Start and stop
  oscillator.start();
  oscillator.stop(audioContext.currentTime + duration);
  
  return oscillator;
}

// Usage
// playTone(440, 'sine', 2); // Play A4 note for 2 seconds
```

### Loading and Playing Audio Files

Let's see how to load an external audio file and play it:

```javascript
async function loadAndPlayAudio(url) {
  try {
    // Fetch the audio file
    const response = await fetch(url);
    const arrayBuffer = await response.arrayBuffer();
    
    // Decode the audio data
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    // Create audio source
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    
    // Connect to destination and play
    source.connect(audioContext.destination);
    source.start();
    
    return source;
  } catch (error) {
    console.error('Error loading audio:', error);
  }
}

// Usage
// loadAndPlayAudio('https://example.com/sound.mp3');
```

## Audio Controls and Effects

### Volume Control with GainNode

The `GainNode` allows us to control the volume of our audio:

```javascript
function setupVolumeControl(audioSource) {
  // Create a gain node
  const gainNode = audioContext.createGain();
  
  // Connect the audio source to the gain node
  audioSource.connect(gainNode);
  
  // Connect the gain node to the destination
  gainNode.connect(audioContext.destination);
  
  // Return the gain node so we can control it later
  return gainNode;
}

// Usage example
const oscillator = audioContext.createOscillator();
const volumeControl = setupVolumeControl(oscillator);

// Change volume (0 = silence, 1 = full volume)
volumeControl.gain.value = 0.5; // 50% volume

oscillator.start();
```

### Adding Filter Effects

Filters allow us to emphasize or suppress certain frequencies:

```javascript
function createFilter(type = 'lowpass', frequency = 1000, Q = 1) {
  const filter = audioContext.createBiquadFilter();
  filter.type = type; // 'lowpass', 'highpass', 'bandpass', etc.
  filter.frequency.value = frequency;
  filter.Q.value = Q;
  return filter;
}

// Example usage: Create a low-pass filter at 800Hz
function playFilteredSound() {
  const oscillator = audioContext.createOscillator();
  oscillator.type = 'sawtooth';
  oscillator.frequency.value = 440;
  
  // Create and configure filter
  const filter = createFilter('lowpass', 800, 5);
  
  // Create the audio graph
  oscillator.connect(filter);
  filter.connect(audioContext.destination);
  
  // Play for 2 seconds
  oscillator.start();
  oscillator.stop(audioContext.currentTime + 2);
}
```

## Audio Analysis and Visualization

One of the most exciting features of the Web Audio API is the ability to analyze audio data in real-time, which we can use to create visualizations.

### Creating a Simple Audio Analyzer

```javascript
function setupAnalyzer(audioSource) {
  // Create analyzer
  const analyzer = audioContext.createAnalyser();
  analyzer.fftSize = 2048;
  
  // Connect source to analyzer
  audioSource.connect(analyzer);
  
  // Also connect source to destination so we can hear it
  audioSource.connect(audioContext.destination);
  
  return analyzer;
}

function drawWaveform(analyzer, canvas) {
  const canvasContext = canvas.getContext('2d');
  const bufferLength = analyzer.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);
  
  function draw() {
    // Schedule next redraw
    requestAnimationFrame(draw);
    
    // Get waveform data
    analyzer.getByteTimeDomainData(dataArray);
    
    // Clear canvas
    canvasContext.clearRect(0, 0, canvas.width, canvas.height);
    
    // Setup drawing
    canvasContext.lineWidth = 2;
    canvasContext.strokeStyle = 'rgb(0, 255, 0)';
    canvasContext.beginPath();
    
    // Draw waveform
    const sliceWidth = canvas.width / bufferLength;
    let x = 0;
    
    for (let i = 0; i < bufferLength; i++) {
      const v = dataArray[i] / 128.0; // normalize to 0-2
      const y = v * (canvas.height / 2);
      
      if (i === 0) {
        canvasContext.moveTo(x, y);
      } else {
        canvasContext.lineTo(x, y);
      }
      
      x += sliceWidth;
    }
    
    canvasContext.lineTo(canvas.width, canvas.height / 2);
    canvasContext.stroke();
  }
  
  draw();
}

// Usage
// const audioElement = document.querySelector('audio');
// const audioSource = audioContext.createMediaElementSource(audioElement);
// const analyzer = setupAnalyzer(audioSource);
// drawWaveform(analyzer, document.getElementById('waveform-canvas'));
```

## Practical Example: Audio Player with Effects

Let's combine what we've learned to create a simple audio player with effects:

```jsx
import React, { useEffect, useRef, useState } from 'react';

function AudioPlayerWithEffects() {
  const [audioContext, setAudioContext] = useState(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [filterFrequency, setFilterFrequency] = useState(1000);
  const [volume, setVolume] = useState(0.7);
  
  const audioSourceRef = useRef(null);
  const gainNodeRef = useRef(null);
  const filterRef = useRef(null);
  
  // Initialize audio context
  const initAudio = () => {
    if (!audioContext) {
      const newContext = new (window.AudioContext || window.webkitAudioContext)();
      setAudioContext(newContext);
      
      // Create nodes
      const source = newContext.createBufferSource();
      const gainNode = newContext.createGain();
      const filter = newContext.createBiquadFilter();
      
      // Configure nodes
      filter.type = 'lowpass';
      filter.frequency.value = filterFrequency;
      gainNode.gain.value = volume;
      
      // Store references
      audioSourceRef.current = source;
      gainNodeRef.current = gainNode;
      filterRef.current = filter;
      
      // Load some audio
      loadAudio(newContext, source, 'https://example.com/audio-sample.mp3');
      
      // Connect graph
      source.connect(filter);
      filter.connect(gainNode);
      gainNode.connect(newContext.destination);
    }
  };
  
  const loadAudio = async (context, source, url) => {
    try {
      const response = await fetch(url);
      const arrayBuffer = await response.arrayBuffer();
      const audioBuffer = await context.decodeAudioData(arrayBuffer);
      source.buffer = audioBuffer;
    } catch (error) {
      console.error('Error loading audio:', error);
    }
  };
  
  // Play/pause
  const togglePlayback = () => {
    if (audioContext) {
      if (isPlaying) {
        audioSourceRef.current.stop();
        setIsPlaying(false);
      } else {
        // Need to create a new source for each play
        const source = audioContext.createBufferSource();
        source.buffer = audioSourceRef.current.buffer;
        source.connect(filterRef.current);
        audioSourceRef.current = source;
        source.start();
        setIsPlaying(true);
      }
    } else {
      initAudio();
    }
  };
  
  // Update filter frequency
  useEffect(() => {
    if (filterRef.current) {
      filterRef.current.frequency.value = filterFrequency;
    }
  }, [filterFrequency]);
  
  // Update volume
  useEffect(() => {
    if (gainNodeRef.current) {
      gainNodeRef.current.gain.value = volume;
    }
  }, [volume]);
  
  return (
    <div>
      <button onClick={togglePlayback}>
        {isPlaying ? 'Pause' : 'Play'}
      </button>
      
      <div>
        <label>
          Filter Frequency: {filterFrequency}Hz
          <input 
            type="range" 
            min="20" 
            max="20000" 
            value={filterFrequency}
            onChange={(e) => setFilterFrequency(Number(e.target.value))}
          />
        </label>
      </div>
      
      <div>
        <label>
          Volume: {Math.round(volume * 100)}%
          <input
            type="range"
            min="0"
            max="1"
            step="0.01"
            value={volume}
            onChange={(e) => setVolume(Number(e.target.value))}
          />
        </label>
      </div>
    </div>
  );
}

export default AudioPlayerWithEffects;
```

## Advanced Examples

### Spatial Audio

The Web Audio API allows us to create 3D audio experiences with spatial positioning:

```javascript
function createSpatialAudio(audioContext, audioBuffer) {
  // Create source and panner nodes
  const source = audioContext.createBufferSource();
  source.buffer = audioBuffer;
  
  const panner = audioContext.createPanner();
  panner.panningModel = 'HRTF'; // Head-related transfer function - more realistic
  panner.distanceModel = 'inverse';
  panner.refDistance = 1;
  panner.maxDistance = 10000;
  panner.rolloffFactor = 1;
  panner.coneInnerAngle = 360;
  panner.coneOuterAngle = 0;
  panner.coneOuterGain = 0;
  
  // Position the sound (x, y, z)
  panner.setPosition(0, 0, 1); // 1 meter away, directly in front
  
  // Connect the nodes
  source.connect(panner);
  panner.connect(audioContext.destination);
  
  return { source, panner };
}

// You could then animate the position:
function moveSoundInCircle(panner, radius = 5) {
  let angle = 0;
  
  function animate() {
    // Calculate new position
    const x = Math.sin(angle) * radius;
    const z = Math.cos(angle) * radius;
    
    // Update panner position
    panner.setPosition(x, 0, z);
    
    // Increment angle
    angle += 0.01;
    
    // Schedule next animation frame
    requestAnimationFrame(animate);
  }
  
  animate();
}
```

### Audio Worklets for Custom Processing

For more advanced audio processing, the Web Audio API provides Audio Worklets:

```javascript
// First, register the processor code in a separate file: 'noise-processor.js'
// noise-processor.js:
class NoiseProcessor extends AudioWorkletProcessor {
  process(inputs, outputs, parameters) {
    const output = outputs[0];
    
    for (let channel = 0; channel < output.length; ++channel) {
      const outputChannel = output[channel];
      for (let i = 0; i < outputChannel.length; ++i) {
        // Generate random noise between -1 and 1
        outputChannel[i] = Math.random() * 2 - 1;
      }
    }
    return true;
  }
}

registerProcessor('noise-processor', NoiseProcessor);

// Then in your main code:
async function setupNoiseGenerator() {
  // Load and register the processor
  await audioContext.audioWorklet.addModule('noise-processor.js');
  
  // Create an instance of the processor
  const noiseGenerator = new AudioWorkletNode(audioContext, 'noise-processor');
  
  // Create a gain node for volume control
  const gainNode = audioContext.createGain();
  gainNode.gain.value = 0.1; // Keep the noise quiet
  
  // Connect
  noiseGenerator.connect(gainNode);
  gainNode.connect(audioContext.destination);
  
  return { noiseGenerator, gainNode };
}
```

## Browser Compatibility and Limitations

The Web Audio API is supported in all modern browsers, but there are some considerations:

1. **Autoplay restrictions**: Most browsers now prevent audio from playing automatically without user interaction.
2. **Mobile considerations**: Mobile browsers may handle audio contexts differently and have additional restrictions.
3. **Legacy browser support**: Some older browsers may require prefixes or not support certain features.

Here's a helper function to handle these issues:

```javascript
function createSafeAudioContext() {
  // Check if Web Audio API is supported
  const AudioContext = window.AudioContext || window.webkitAudioContext;
  
  if (!AudioContext) {
    console.error('Web Audio API is not supported in this browser');
    return null;
  }
  
  // Create audio context
  const audioContext = new AudioContext();
  
  // On iOS, audio context starts in suspended state
  if (audioContext.state === 'suspended') {
    const resumeAudio = function() {
      audioContext.resume();
      
      // Remove the event listeners once audio is resumed
      ['touchstart', 'touchend', 'mousedown', 'keydown'].forEach(event => {
        document.body.removeEventListener(event, resumeAudio);
      });
      
      console.log('Audio context resumed');
    };
    
    ['touchstart', 'touchend', 'mousedown', 'keydown'].forEach(event => {
      document.body.addEventListener(event, resumeAudio);
    });
  }
  
  return audioContext;
}
```

## Summary

In this tutorial, we've explored the fundamentals of the Web Audio API:

- Creating an audio context - the foundation of Web Audio API
- Building audio graphs by connecting audio nodes
- Generating sounds with oscillators
- Loading and playing external audio files
- Adding effects like filters and volume controls
- Analyzing and visualizing audio data
- Creating spatial audio experiences
- Using Audio Worklets for custom processing

The Web Audio API provides powerful capabilities for audio processing in web applications, enabling everything from simple sound effects to complete digital audio workstations right in the browser.

## Exercises

1. **Simple Synthesizer**: Create a keyboard synthesizer that plays different notes when pressing keyboard keys.
2. **Audio Visualizer**: Build a music player that displays a frequency spectrum visualization of the audio.
3. **Drum Machine**: Create a drum machine that plays different percussion sounds when clicking on buttons.
4. **Sound Spatializer**: Make a demo where users can position sounds in 3D space around them.
5. **Voice Effects**: Create an application that applies different effects (pitch shifting, reverb, echo) to microphone input.

## Additional Resources

1. [MDN Web Audio API Guide](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
2. [Web Audio API Specification](https://webaudio.github.io/web-audio-api/)
3. [Web Audio Weekly Newsletter](https://www.webaudioweekly.com/)
4. [Tone.js](https://tonejs.github.io/) - A framework for creating interactive music in the browser
5. [Introduction to Web Audio API by Boris Smus](https://www.html5rocks.com/en/tutorials/webaudio/intro/)

Happy audio coding!