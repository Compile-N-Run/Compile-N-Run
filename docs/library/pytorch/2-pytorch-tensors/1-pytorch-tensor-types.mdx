---
title: PyTorch Tensor Types
description: Learn about the different types of tensors in PyTorch, their properties, and when to use each one.

---

# PyTorch Tensor Types

In PyTorch, tensors are the fundamental data structures used to store and manipulate data. Similar to NumPy arrays, tensors can have different data types that determine how the data is stored and processed. Understanding tensor types is crucial for efficient memory usage, computational performance, and ensuring compatibility across operations.

## Introduction to Tensor Data Types

A tensor's data type defines the kind of values it can hold and how much memory each element occupies. PyTorch supports multiple data types to accommodate various needs in machine learning applications.

Let's explore the most common tensor types in PyTorch:

## Numeric Data Types

### Floating Point Types

Floating point types are used to represent real numbers with decimal points.

```python
import torch

# Float32 (default)
float_tensor = torch.tensor([1.0, 2.0, 3.0])
print(f"Default float tensor: {float_tensor.dtype}")

# Float64 (double precision)
double_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)
print(f"Double tensor: {double_tensor.dtype}")

# Float16 (half precision)
half_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float16)
print(f"Half tensor: {half_tensor.dtype}")
```

Output:
```
Default float tensor: torch.float32
Double tensor: torch.float64
Half tensor: torch.float16
```

#### When to use different float types:

- `torch.float32` (or `torch.float`): Default floating-point type, balances precision and efficiency
- `torch.float64` (or `torch.double`): Higher precision, but uses more memory and can be slower
- `torch.float16` (or `torch.half`): Lower precision, but saves memory and can be faster, especially on modern GPUs with specialized hardware

### Integer Types

Integer types store whole numbers without decimal parts.

```python
import torch

# Integer types
int_tensor = torch.tensor([1, 2, 3])  # Default is int64
print(f"Default integer tensor: {int_tensor.dtype}")

int32_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)
print(f"Int32 tensor: {int32_tensor.dtype}")

int16_tensor = torch.tensor([1, 2, 3], dtype=torch.int16)
print(f"Int16 tensor: {int16_tensor.dtype}")

int8_tensor = torch.tensor([1, 2, 3], dtype=torch.int8)
print(f"Int8 tensor: {int8_tensor.dtype}")
```

Output:
```
Default integer tensor: torch.int64
Int32 tensor: torch.int32
Int16 tensor: torch.int16
Int8 tensor: torch.int8
```

#### When to use different integer types:

- `torch.int64` (or `torch.long`): Default integer type, preferred for indexing
- `torch.int32` (or `torch.int`): Good balance for integer calculations
- `torch.int16` (or `torch.short`): Saves memory when you need only moderate range
- `torch.int8`: For very small integers (range -128 to 127), saves memory

### Boolean Type

Boolean tensors store True/False values.

```python
import torch

# Boolean tensor
bool_tensor = torch.tensor([True, False, True])
print(f"Boolean tensor: {bool_tensor.dtype}")
print(f"Values: {bool_tensor}")

# Creating boolean tensor from comparison
comparison = torch.tensor([1, 2, 3]) > 1
print(f"Comparison result type: {comparison.dtype}")
print(f"Comparison values: {comparison}")
```

Output:
```
Boolean tensor: torch.bool
Values: tensor([ True, False,  True])
Comparison result type: torch.bool
Comparison values: tensor([False,  True,  True])
```

## Complex Types

PyTorch supports complex numbers with two types:

```python
import torch

# Complex tensors
complex64 = torch.tensor([1+2j, 3+4j], dtype=torch.complex64)
print(f"Complex64 tensor: {complex64.dtype}")
print(f"Values: {complex64}")

complex128 = torch.tensor([1+2j, 3+4j], dtype=torch.complex128)
print(f"Complex128 tensor: {complex128.dtype}")
print(f"Values: {complex128}")
```

Output:
```
Complex64 tensor: torch.complex64
Values: tensor([1.+2.j, 3.+4.j])
Complex128 tensor: torch.complex128
Values: tensor([1.+2.j, 3.+4.j])
```

Complex types are particularly useful for signal processing, Fourier transforms, and certain mathematical operations.

## Quantized Types

PyTorch also supports quantized types, which are lower-precision representations that can significantly reduce memory usage and computational requirements. These are especially useful for model deployment on resource-constrained devices.

```python
import torch

# Create a regular float tensor
float_tensor = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float)

# Quantize to int8
q_tensor = torch.quantize_per_tensor(float_tensor, scale=0.1, zero_point=10, dtype=torch.qint8)
print(f"Quantized tensor type: {q_tensor.dtype}")
print(f"Quantized tensor: {q_tensor}")

# Dequantize back to float
dq_tensor = q_tensor.dequantize()
print(f"Dequantized tensor: {dq_tensor}")
```

Output:
```
Quantized tensor type: torch.qint8
Quantized tensor: tensor([ 1., -2.,  3., -4.], size=(4,), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
Dequantized tensor: tensor([ 1.0000, -2.0000,  3.0000, -4.0000])
```

## Converting Between Tensor Types

PyTorch provides easy ways to convert between different types:

```python
import torch

# Create a tensor
original = torch.tensor([1.7, 2.3, 3.9])
print(f"Original tensor: {original}, type: {original.dtype}")

# Convert to different types
to_int = original.int()  # Truncates decimal part
print(f"Converted to int: {to_int}, type: {to_int.dtype}")

to_long = original.long()
print(f"Converted to long: {to_long}, type: {to_long.dtype}")

to_float16 = original.half()
print(f"Converted to half: {to_float16}, type: {to_float16.dtype}")

to_double = original.double()
print(f"Converted to double: {to_double}, type: {to_double.dtype}")

# Using the to() method
to_bool = original.to(torch.bool)  # Non-zero values become True
print(f"Converted to bool: {to_bool}, type: {to_bool.dtype}")
```

Output:
```
Original tensor: tensor([1.7000, 2.3000, 3.9000]), type: torch.float32
Converted to int: tensor([1, 2, 3]), type: torch.int32
Converted to long: tensor([1, 2, 3]), type: torch.int64
Converted to half: tensor([1.7002, 2.2998, 3.8984], dtype=torch.float16), type: torch.float16
Converted to double: tensor([1.7000, 2.3000, 3.9000], dtype=torch.float64), type: torch.float64
Converted to bool: tensor([True, True, True]), type: torch.bool
```

### Be Careful with Type Conversions

When converting between types, be aware of potential issues:

1. **Truncation**: Converting from floating-point to integer truncates the decimal part
2. **Overflow**: Converting to a smaller data type might result in overflow if values exceed the range
3. **Precision loss**: Converting from higher precision to lower precision (e.g., float64 to float32) may lose precision

## Default Types and Changing Defaults

PyTorch has default types for tensor creation:

```python
import torch

# Default types
print(f"Default floating point type: {torch.get_default_dtype()}")

# Create tensors with default type
x = torch.tensor([1.0, 2.0, 3.0])
print(f"Default tensor type: {x.dtype}")

# Change default type
torch.set_default_dtype(torch.float64)
y = torch.tensor([1.0, 2.0, 3.0])
print(f"New default tensor type: {y.dtype}")

# Reset to original default
torch.set_default_dtype(torch.float32)
```

Output:
```
Default floating point type: torch.float32
Default tensor type: torch.float32
New default tensor type: torch.float64
```

## Practical Applications

### Memory Optimization

Different tensor types use different amounts of memory:

```python
import torch

# Create tensors of different types
int8 = torch.ones(1000, 1000, dtype=torch.int8)
int16 = torch.ones(1000, 1000, dtype=torch.int16)
int32 = torch.ones(1000, 1000, dtype=torch.int32)
int64 = torch.ones(1000, 1000, dtype=torch.int64)
float16 = torch.ones(1000, 1000, dtype=torch.float16)
float32 = torch.ones(1000, 1000, dtype=torch.float32)
float64 = torch.ones(1000, 1000, dtype=torch.float64)

# Compare memory usage (in MB)
print(f"int8: {int8.element_size() * int8.numel() / 1e6} MB")
print(f"int16: {int16.element_size() * int16.numel() / 1e6} MB")
print(f"int32: {int32.element_size() * int32.numel() / 1e6} MB")
print(f"int64: {int64.element_size() * int64.numel() / 1e6} MB")
print(f"float16: {float16.element_size() * float16.numel() / 1e6} MB")
print(f"float32: {float32.element_size() * float32.numel() / 1e6} MB")
print(f"float64: {float64.element_size() * float64.numel() / 1e6} MB")
```

Output:
```
int8: 1.0 MB
int16: 2.0 MB
int32: 4.0 MB
int64: 8.0 MB
float16: 2.0 MB
float32: 4.0 MB
float64: 8.0 MB
```

For large models, choosing appropriate tensor types can significantly reduce memory usage.

### Mixed Precision Training

Modern deep learning often uses mixed precision training, combining float16 and float32 types for better performance:

```python
import torch

# Simple mixed precision example
def mixed_precision_example():
    # Create a model (just a simple linear layer for demonstration)
    model = torch.nn.Linear(1000, 1000)
    
    # Convert model to half precision
    model.half()
    
    # Input in half precision
    inputs = torch.randn(100, 1000, dtype=torch.float16)
    
    # Forward pass in half precision
    outputs = model(inputs)
    
    # Loss computation might be kept in float32 for stability
    loss = outputs.float().mean()
    
    print(f"Input dtype: {inputs.dtype}")
    print(f"Output dtype: {outputs.dtype}")
    print(f"Loss dtype: {loss.dtype}")
    
    return loss

loss = mixed_precision_example()
```

Output:
```
Input dtype: torch.float16
Output dtype: torch.float16
Loss dtype: torch.float32
```

### Type Compatibility in Operations

When performing operations with tensors of different types, PyTorch follows type promotion rules:

```python
import torch

# Mixing types in operations
float_tensor = torch.tensor([1.0, 2.0, 3.0])
int_tensor = torch.tensor([1, 2, 3])

# Addition follows type promotion rules
result = float_tensor + int_tensor
print(f"Result dtype: {result.dtype}")
print(f"Result: {result}")

# Matrix multiplication with different types
a = torch.randn(2, 3, dtype=torch.float32)
b = torch.randn(3, 2, dtype=torch.float64)

try:
    c = a @ b
    print(f"Matrix multiplication result dtype: {c.dtype}")
except RuntimeError as e:
    print(f"Error: {e}")
    print("Need to convert to the same type first:")
    c = a.double() @ b  # or b.float() @ a.t()
    print(f"Matrix multiplication result dtype: {c.dtype}")
```

Output:
```
Result dtype: torch.float32
Result: tensor([2., 4., 6.])
Error: expected scalar type Float but found Double
Need to convert to the same type first:
Matrix multiplication result dtype: torch.float64
```

## GPU Considerations

Different GPUs have different optimal tensor types:

```python
import torch

# Check if CUDA is available
if torch.cuda.is_available():
    # Create tensors on GPU
    float32_gpu = torch.ones(1000, 1000, dtype=torch.float32, device='cuda')
    float16_gpu = torch.ones(1000, 1000, dtype=torch.float16, device='cuda')
    
    # Time simple operations
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    # Test float32
    start.record()
    for _ in range(100):
        result = float32_gpu @ float32_gpu
    end.record()
    torch.cuda.synchronize()
    print(f"Float32 matrix multiplication time: {start.elapsed_time(end):.2f} ms")
    
    # Test float16
    start.record()
    for _ in range(100):
        result = float16_gpu @ float16_gpu
    end.record()
    torch.cuda.synchronize()
    print(f"Float16 matrix multiplication time: {start.elapsed_time(end):.2f} ms")
else:
    print("CUDA not available. This example requires a GPU.")
```

Note: The output will vary based on your GPU, but on compatible GPUs, float16 operations can be significantly faster.

## Summary

PyTorch tensor types provide flexibility for various needs in deep learning:

- **Floating point types** (`float32`, `float64`, `float16`) balance precision and efficiency
- **Integer types** (`int64`, `int32`, etc.) handle whole numbers with different ranges
- **Boolean types** store binary values and are useful for masking and conditions
- **Complex types** handle complex numbers for specialized mathematical operations
- **Quantized types** reduce memory and compute requirements for deployment

Choosing the right tensor type can have significant impacts on:
1. Memory usage
2. Computation speed
3. Numerical precision/stability
4. Hardware compatibility

For most beginners, the default types (`float32` for floating point and `int64` for integers) work well, but as you advance in deep learning, understanding and leveraging different tensor types becomes increasingly important.

## Additional Resources

1. [PyTorch Documentation on Tensor Types](https://pytorch.org/docs/stable/tensors.html)
2. [Mixed Precision Training Tutorial](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
3. [Quantization in PyTorch](https://pytorch.org/docs/stable/quantization.html)

## Exercises

1. Create tensors of different types and compare their memory usage for a matrix of size 5000×5000.
2. Write a function that safely converts between different tensor types, handling potential overflow and underflow issues.
3. Experiment with mixed precision training on a simple neural network and compare training time against standard precision.
4. Create a complex tensor and use it to perform a Fast Fourier Transform (FFT) on a simple signal.
5. Investigate how using different tensor types affects the accuracy of a simple neural network on the MNIST dataset.