---
title: PyTorch Tensor Reshaping
description: Learn various methods to reshape PyTorch tensors including view, reshape, squeeze, unsqueeze, and transpose operations with clear examples and practical applications.

---

# PyTorch Tensor Reshaping

Reshaping tensors is a fundamental operation in deep learning and neural network implementations. When working with PyTorch, you'll often need to change the dimensions of your tensors to make them compatible with different operations or model architectures. This guide covers the essential tensor reshaping operations in PyTorch, with practical examples and explanations.

## Introduction to Tensor Reshaping

Tensor reshaping refers to changing the dimensions of a tensor without altering its data. Think of it as rearranging the elements of a tensor to fit into a new shape. The total number of elements remains the same; only the arrangement changes.

Let's start by importing PyTorch:

```python
import torch
```

## Basic Reshaping Operations

### The `view()` Method

The `view()` method is the most common way to reshape a tensor in PyTorch. It returns a new tensor with the same data but a different shape.

```python
# Create a 2D tensor
x = torch.tensor([[1, 2, 3], 
                  [4, 5, 6]])
print("Original tensor:")
print(x)
print("Shape:", x.shape)

# Reshape to a 1D tensor
y = x.view(6)
print("\nReshaped to 1D:")
print(y)
print("Shape:", y.shape)

# Reshape to a 3x2 tensor
z = x.view(3, 2)
print("\nReshaped to 3x2:")
print(z)
print("Shape:", z.shape)
```

Output:
```
Original tensor:
tensor([[1, 2, 3],
        [4, 5, 6]])
Shape: torch.Size([2, 3])

Reshaped to 1D:
tensor([1, 2, 3, 4, 5, 6])
Shape: torch.Size([6])

Reshaped to 3x2:
tensor([[1, 2],
        [3, 4],
        [5, 6]])
Shape: torch.Size([3, 2])
```

### Using `-1` in Reshaping

The `-1` parameter in `view()` is particularly useful when you want PyTorch to automatically calculate one dimension based on the others:

```python
tensor = torch.arange(24)  # Create a tensor with values 0 to 23

# Reshape to 2x3x4
reshaped = tensor.view(2, 3, 4)
print("Reshaped to 2x3x4:")
print(reshaped)
print("Shape:", reshaped.shape)

# Use -1 to automatically determine the first dimension
auto_reshaped = tensor.view(-1, 3, 4)
print("\nReshaped with automatic first dimension:")
print(auto_reshaped)
print("Shape:", auto_reshaped.shape)
```

Output:
```
Reshaped to 2x3x4:
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
Shape: torch.Size([2, 3, 4])

Reshaped with automatic first dimension:
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
Shape: torch.Size([2, 3, 4])
```

### The `reshape()` Method

The `reshape()` method is similar to `view()` but may return a copy of the tensor if necessary:

```python
x = torch.tensor([[1, 2, 3], 
                 [4, 5, 6]])

# Using reshape
reshaped = x.reshape(3, 2)
print("Reshaped tensor:")
print(reshaped)
print("Shape:", reshaped.shape)
```

Output:
```
Reshaped tensor:
tensor([[1, 2],
        [3, 4],
        [5, 6]])
Shape: torch.Size([3, 2])
```

**Key Difference**: `view()` and `reshape()` differ in how they handle memory. `view()` requires the tensor to be contiguous in memory, while `reshape()` will make a copy if necessary.

## Advanced Reshaping Operations

### `squeeze()` - Removing Dimensions of Size 1

The `squeeze()` method removes dimensions with a size of 1:

```python
# Create a tensor with a singleton dimension
x = torch.tensor([[[1], [2], [3]]])
print("Original tensor:")
print(x)
print("Shape:", x.shape)  # Shape: [1, 3, 1]

# Remove all singleton dimensions
squeezed = x.squeeze()
print("\nAfter squeeze():")
print(squeezed)
print("Shape:", squeezed.shape)  # Shape: [3]

# Remove specific singleton dimension
squeezed_dim = x.squeeze(0)
print("\nAfter squeeze(0):")
print(squeezed_dim)
print("Shape:", squeezed_dim.shape)  # Shape: [3, 1]
```

Output:
```
Original tensor:
tensor([[[1],
         [2],
         [3]]])
Shape: torch.Size([1, 3, 1])

After squeeze():
tensor([1, 2, 3])
Shape: torch.Size([3])

After squeeze(0):
tensor([[1],
        [2],
        [3]])
Shape: torch.Size([3, 1])
```

### `unsqueeze()` - Adding Singleton Dimensions

The `unsqueeze()` method adds a dimension of size 1 at a specified position:

```python
# Create a 1D tensor
x = torch.tensor([1, 2, 3, 4])
print("Original tensor:")
print(x)
print("Shape:", x.shape)  # Shape: [4]

# Add a dimension at index 0
unsqueezed0 = x.unsqueeze(0)
print("\nAfter unsqueeze(0):")
print(unsqueezed0)
print("Shape:", unsqueezed0.shape)  # Shape: [1, 4]

# Add a dimension at index 1
unsqueezed1 = x.unsqueeze(1)
print("\nAfter unsqueeze(1):")
print(unsqueezed1)
print("Shape:", unsqueezed1.shape)  # Shape: [4, 1]
```

Output:
```
Original tensor:
tensor([1, 2, 3, 4])
Shape: torch.Size([4])

After unsqueeze(0):
tensor([[1, 2, 3, 4]])
Shape: torch.Size([1, 4])

After unsqueeze(1):
tensor([[1],
        [2],
        [3],
        [4]])
Shape: torch.Size([4, 1])
```

### Transposing Tensors with `transpose()` and `permute()`

#### `transpose()` - Swap Two Dimensions

```python
# Create a 2D tensor
x = torch.tensor([[1, 2, 3], 
                  [4, 5, 6]])
print("Original tensor:")
print(x)
print("Shape:", x.shape)  # Shape: [2, 3]

# Transpose dimensions 0 and 1
transposed = x.transpose(0, 1)
print("\nTransposed tensor:")
print(transposed)
print("Shape:", transposed.shape)  # Shape: [3, 2]
```

Output:
```
Original tensor:
tensor([[1, 2, 3],
        [4, 5, 6]])
Shape: torch.Size([2, 3])

Transposed tensor:
tensor([[1, 4],
        [2, 5],
        [3, 6]])
Shape: torch.Size([3, 2])
```

#### `permute()` - Rearrange Multiple Dimensions

For tensors with more than 2 dimensions, `permute()` allows you to rearrange dimensions in any order:

```python
# Create a 3D tensor
x = torch.randn(2, 3, 4)  # Shape: [2, 3, 4]
print("Original shape:", x.shape)

# Permute dimensions to [4, 2, 3]
permuted = x.permute(2, 0, 1)
print("Permuted shape:", permuted.shape)
```

Output:
```
Original shape: torch.Size([2, 3, 4])
Permuted shape: torch.Size([4, 2, 3])
```

## Flattening and Unflatten Tensors

### Flattening with `flatten()`

The `flatten()` method collapses specified dimensions into one:

```python
# Create a 3D tensor
x = torch.tensor([[[1, 2], [3, 4]], 
                  [[5, 6], [7, 8]]])
print("Original tensor:")
print(x)
print("Shape:", x.shape)  # Shape: [2, 2, 2]

# Flatten all dimensions
flattened = x.flatten()
print("\nCompletely flattened:")
print(flattened)
print("Shape:", flattened.shape)  # Shape: [8]

# Flatten starting from dimension 1
partly_flattened = x.flatten(start_dim=1)
print("\nPartially flattened (from dim 1):")
print(partly_flattened)
print("Shape:", partly_flattened.shape)  # Shape: [2, 4]
```

Output:
```
Original tensor:
tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
Shape: torch.Size([2, 2, 2])

Completely flattened:
tensor([1, 2, 3, 4, 5, 6, 7, 8])
Shape: torch.Size([8])

Partially flattened (from dim 1):
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])
Shape: torch.Size([2, 4])
```

## Practical Examples of Tensor Reshaping

### Example 1: Image Processing

In computer vision, images are typically represented as 3D tensors with dimensions `[height, width, channels]`. However, many neural networks expect inputs in the format `[batch_size, channels, height, width]`:

```python
# Simulate an RGB image (height=28, width=28, channels=3)
image = torch.randn(28, 28, 3)
print("Original image shape:", image.shape)

# Reshape for neural network input (add batch dimension and move channels)
# First, add a batch dimension with unsqueeze
image_batch = image.unsqueeze(0)  # Shape: [1, 28, 28, 3]
print("After adding batch dimension:", image_batch.shape)

# Now permute to get [batch_size, channels, height, width]
image_for_nn = image_batch.permute(0, 3, 1, 2)
print("Ready for neural network:", image_for_nn.shape)
```

Output:
```
Original image shape: torch.Size([28, 28, 3])
After adding batch dimension: torch.Size([1, 28, 28, 3])
Ready for neural network: torch.Size([1, 3, 28, 28])
```

### Example 2: Handling Mini-Batches in RNNs

When working with recurrent neural networks (RNNs), you often need to reshape your data to match the expected input format `[sequence_length, batch_size, features]`:

```python
# Create a batch of time series data: [batch_size, sequence_length, features]
batch_data = torch.randn(32, 10, 5)  # 32 samples, 10 time steps, 5 features
print("Original batch shape:", batch_data.shape)

# Transpose to get [sequence_length, batch_size, features]
rnn_input = batch_data.permute(1, 0, 2)
print("Shape for RNN input:", rnn_input.shape)
```

Output:
```
Original batch shape: torch.Size([32, 10, 5])
Shape for RNN input: torch.Size([10, 32, 5])
```

### Example 3: Flattening Features for Linear Layers

When transitioning from convolutional layers to linear (fully-connected) layers in neural networks, you'll need to flatten the spatial dimensions:

```python
# Simulate output from a convolutional layer
# [batch_size, channels, height, width]
conv_output = torch.randn(64, 128, 7, 7)
print("Conv output shape:", conv_output.shape)

# Flatten all dimensions except batch
flattened_features = conv_output.flatten(start_dim=1)
print("Flattened for linear layer:", flattened_features.shape)
```

Output:
```
Conv output shape: torch.Size([64, 128, 7, 7])
Flattened for linear layer: torch.Size([64, 6272])
```

## Common Issues and Best Practices

### Contiguous vs. Non-Contiguous Tensors

After certain operations like `transpose()`, tensors may become non-contiguous in memory, which can cause issues with `view()`:

```python
x = torch.tensor([[1, 2, 3], 
                 [4, 5, 6]])

# Transpose the tensor
transposed = x.transpose(0, 1)  # This creates a non-contiguous tensor
print("Is transposed contiguous?", transposed.is_contiguous())

try:
    # This will raise an error for non-contiguous tensors
    reshaped = transposed.view(-1)
except RuntimeError as e:
    print(f"Error: {e}")
    
# Solution: make it contiguous before using view
contiguous_tensor = transposed.contiguous()
reshaped = contiguous_tensor.view(-1)
print("Reshaped successfully:", reshaped)
```

Output:
```
Is transposed contiguous? False
Error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
Reshaped successfully: tensor([1, 4, 2, 5, 3, 6])
```

### Memory Efficiency

`view()` is memory-efficient since it doesn't create a copy of the data, just a new way of looking at it. Use it when possible:

```python
# Using view (no copy)
original = torch.randn(10, 20)
view_tensor = original.view(200)  # No new memory allocated

# Changing the view affects the original
view_tensor[0] = 999
print("Original tensor after modifying view:", original[0, 0])  # Prints 999
```

## Summary

Reshaping operations are crucial for manipulating tensor dimensions in PyTorch. In this guide, we've covered:

1. Basic reshaping with `view()` and `reshape()`
2. Adding and removing dimensions with `squeeze()` and `unsqueeze()`
3. Rearranging dimensions with `transpose()` and `permute()`
4. Flattening tensors with `flatten()`
5. Practical examples in deep learning contexts
6. Common issues and best practices

Mastering these operations will help you handle tensors efficiently when building and working with neural networks in PyTorch.

## Exercises

1. Create a 3D tensor with shape `[2, 3, 4]` and reshape it to `[4, 6]` using both `view()` and `reshape()`.
2. Take a tensor with shape `[8, 1, 6, 1]` and remove all singleton dimensions.
3. Convert a batch of 16 images with shape `[16, 3, 32, 32]` (batch, channels, height, width) to grayscale with shape `[16, 1, 32, 32]`.
4. Create a tensor of shape `[3, 4, 5]` and rearrange its dimensions to `[5, 3, 4]`.
5. Implement a function that reshapes the output of a convolutional layer to be fed into an LSTM layer.

## Additional Resources

- [PyTorch Documentation on Tensor Views](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view)
- [PyTorch Documentation on Reshaping Operations](https://pytorch.org/docs/stable/generated/torch.reshape.html)
- [Understanding Tensor Reshaping in Deep Learning](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)
- [PyTorch Forums on Tensor Manipulation](https://discuss.pytorch.org/)

Happy reshaping!