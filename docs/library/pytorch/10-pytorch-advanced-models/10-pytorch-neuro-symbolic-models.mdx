---
title: PyTorch Neuro-Symbolic Models
description: Learn how to combine neural networks with symbolic reasoning in PyTorch to create powerful neuro-symbolic models that leverage both data-driven learning and logical reasoning.

---

# PyTorch Neuro-Symbolic Models

In the realm of artificial intelligence, there's a growing movement to combine the strengths of neural networks with symbolic reasoning systems. This fusion, known as **neuro-symbolic AI**, aims to leverage both the pattern recognition capabilities of neural networks and the logical reasoning abilities of symbolic systems.

## Introduction to Neuro-Symbolic AI

Traditional deep learning excels at pattern recognition but struggles with logical reasoning and abstraction. Conversely, symbolic AI systems are excellent at logical reasoning but lack the ability to learn from raw data. Neuro-symbolic models aim to bridge this gap.

These hybrid systems typically have:

- A neural component that processes raw data
- A symbolic component that handles reasoning
- An interface that connects these two components

PyTorch offers flexibility that makes it well-suited for implementing neuro-symbolic architectures. Let's explore how to build such models.

## Core Concepts in Neuro-Symbolic AI

Before diving into code, let's understand some key concepts:

1. **Neural Networks**: Data-driven models that learn patterns from examples
2. **Symbolic Systems**: Rule-based systems that use logical operations
3. **Symbol Grounding**: Connecting neural representations to symbolic meanings
4. **Logical Reasoning**: Applying rules of inference to arrive at conclusions

## Building a Simple Neuro-Symbolic Model in PyTorch

Let's start with a basic example where we use a neural network to process images, then feed the output into a symbolic reasoning system.

### Step 1: Set up the environment

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import numpy as np
```

### Step 2: Define the neural component

```python
class ImageEncoder(nn.Module):
    def __init__(self):
        super(ImageEncoder, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)  # Output 10 symbolic features
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

### Step 3: Define the symbolic reasoning component

For our example, we'll create a simple rule-based system that interprets the neural network's outputs:

```python
class SymbolicReasoner:
    def __init__(self, threshold=0.5):
        self.threshold = threshold
        self.rules = {
            0: "round object",
            1: "contains straight lines",
            2: "contains curves",
            3: "has symmetry",
            4: "has enclosed spaces",
            # ... and so on
        }
        
    def apply_rules(self, neural_output):
        # Convert neural outputs to binary symbols based on threshold
        binary_features = (neural_output > self.threshold).float()
        
        # Apply symbolic reasoning
        conclusions = []
        for i, activated in enumerate(binary_features):
            if activated:
                conclusions.append(self.rules.get(i, f"feature_{i}"))
                
        return conclusions
```

### Step 4: Create the neuro-symbolic interface

```python
class NeuroSymbolicModel:
    def __init__(self, neural_model, symbolic_reasoner):
        self.neural_model = neural_model
        self.symbolic_reasoner = symbolic_reasoner
        
    def forward(self, x):
        # Get neural embeddings
        neural_output = self.neural_model(x)
        
        # Apply symbolic reasoning
        conclusions = []
        for output in neural_output:
            conclusion = self.symbolic_reasoner.apply_rules(output)
            conclusions.append(conclusion)
            
        return neural_output, conclusions
```

### Step 5: Use the model

```python
# Initialize components
encoder = ImageEncoder()
reasoner = SymbolicReasoner(threshold=0.7)
neuro_symbolic_model = NeuroSymbolicModel(encoder, reasoner)

# Create a dummy input (batch of 2 grayscale images)
sample_input = torch.randn(2, 1, 28, 28)

# Process through the neuro-symbolic model
neural_output, symbolic_conclusions = neuro_symbolic_model.forward(sample_input)

print("Neural Output (features):")
print(neural_output)

print("\nSymbolic Conclusions:")
for i, conclusion in enumerate(symbolic_conclusions):
    print(f"Image {i}: {conclusion}")
```

**Example Output:**
```
Neural Output (features):
tensor([[ 0.2143,  0.8654, -0.3251,  0.7123,  0.1432,  0.9123, -0.2134,  0.3421,
         -0.5432,  0.2345],
        [-0.1234,  0.7654,  0.8123,  0.3456, -0.2345,  0.8765,  0.2345, -0.1234,
          0.5678,  0.3456]], grad_fn=<AddmmBackward0>)

Symbolic Conclusions:
Image 0: ['contains straight lines', 'has symmetry', 'has enclosed spaces']
Image 1: ['contains straight lines', 'contains curves', 'has enclosed spaces']
```

## Differentiable Logic in PyTorch

One of the challenges in neuro-symbolic AI is making the symbolic components differentiable so they can be trained end-to-end with neural networks. Let's implement a simple differentiable logic layer:

```python
class DifferentiableLogicLayer(nn.Module):
    def __init__(self, input_dim, num_rules):
        super(DifferentiableLogicLayer, self).__init__()
        self.weights = nn.Parameter(torch.randn(input_dim, num_rules))
        
    def forward(self, x):
        # Implement fuzzy AND operation (minimum)
        conjunction = torch.min(x.unsqueeze(-1), self.weights)
        # Implement fuzzy OR operation (maximum)
        disjunction = torch.max(torch.zeros_like(conjunction), conjunction)
        # Aggregate over features
        rule_activations = torch.sum(disjunction, dim=1)
        return torch.sigmoid(rule_activations)
```

### Using the differentiable logic layer:

```python
# Neural encoder
encoder = ImageEncoder()

# Differentiable logic layer (3 symbolic rules)
logic_layer = DifferentiableLogicLayer(10, 3)

# Process data through both components
sample_input = torch.randn(2, 1, 28, 28)
neural_features = encoder(sample_input)
rule_outputs = logic_layer(neural_features)

print("Neural Features:")
print(neural_features)
print("\nRule Activations:")
print(rule_outputs)
```

**Example Output:**
```
Neural Features:
tensor([[ 0.2345,  0.8765, -0.3456,  0.7654,  0.1234,  0.9876, -0.2345,  0.3456,
         -0.5678,  0.2345],
        [-0.1234,  0.7654,  0.8765,  0.3456, -0.2345,  0.8765,  0.2345, -0.1234,
          0.5678,  0.3456]], grad_fn=<AddmmBackward0>)

Rule Activations:
tensor([[0.7612, 0.8234, 0.5432],
        [0.6543, 0.8765, 0.7123]], grad_fn=<SigmoidBackward0>)
```

## Real-World Application: Visual Question Answering

Let's build a simple neuro-symbolic model for visual question answering (VQA) that can understand both the content of an image and answer questions about it:

```python
class NeuroSymbolicVQA(nn.Module):
    def __init__(self):
        super(NeuroSymbolicVQA, self).__init__()
        # Image encoder (neural)
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(32 * 8 * 8, 128)
        )
        
        # Question encoder (neural)
        self.question_encoder = nn.Sequential(
            nn.Embedding(1000, 64),  # Vocabulary size of 1000
            nn.LSTM(64, 128, batch_first=True),
        )
        
        # Concept grounding (interface between neural and symbolic)
        self.concept_grounding = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 20)  # 20 symbolic concepts
        )
        
        # Reasoning module (symbolic)
        self.reasoning_rules = {
            "color": [0, 1, 2],       # Concepts 0, 1, 2 represent colors
            "shape": [3, 4, 5, 6],    # Concepts 3-6 represent shapes
            "location": [7, 8, 9, 10], # Concepts 7-10 represent locations
            # More rules can be added
        }
        
        # Answer generator
        self.answer_generator = nn.Linear(20, 100)  # 100 possible answers
        
    def forward(self, image, question, question_type):
        # Neural processing
        img_features = self.image_encoder(image)
        
        # For the question, we need to handle the LSTM output properly
        question_packed = nn.utils.rnn.pack_padded_sequence(
            question, [question.size(1)]*question.size(0), batch_first=True, enforce_sorted=False
        )
        _, (question_features, _) = self.question_encoder(question_packed)
        question_features = question_features.squeeze(0)
        
        # Combine features
        combined = torch.cat([img_features, question_features], dim=1)
        
        # Concept grounding (neural to symbolic)
        concepts = torch.sigmoid(self.concept_grounding(combined))
        
        # Symbolic reasoning based on question type
        relevant_concepts = torch.zeros_like(concepts)
        
        if question_type == "color":
            for idx in self.reasoning_rules["color"]:
                relevant_concepts[:, idx] = concepts[:, idx]
        elif question_type == "shape":
            for idx in self.reasoning_rules["shape"]:
                relevant_concepts[:, idx] = concepts[:, idx]
        elif question_type == "location":
            for idx in self.reasoning_rules["location"]:
                relevant_concepts[:, idx] = concepts[:, idx]
        
        # Generate answer
        logits = self.answer_generator(relevant_concepts)
        return logits, concepts
```

### Using the VQA Model:

```python
# Initialize model
vqa_model = NeuroSymbolicVQA()

# Create dummy inputs
batch_size = 2
images = torch.randn(batch_size, 3, 32, 32)
questions = torch.randint(0, 1000, (batch_size, 10))
question_types = ["color", "shape"]

# Forward pass
logits, concepts = vqa_model(images, questions, question_types[0])

print("Activated Concepts:")
print(concepts)
print("\nAnswer Logits:")
print(logits)
```

## Benefits of Neuro-Symbolic Models

1. **Explainability**: The symbolic component provides clear reasoning steps.
2. **Data Efficiency**: Requires less training data by incorporating prior knowledge.
3. **Logical Consistency**: Can enforce logical constraints that neural networks alone might violate.
4. **Knowledge Integration**: Can incorporate existing knowledge bases and ontologies.

## Advanced Techniques in Neuro-Symbolic AI

### 1. Logic Tensor Networks

Logic Tensor Networks (LTNs) combine neural networks with fuzzy logic. Let's implement a simple LTN module:

```python
class LogicTensorNetwork(nn.Module):
    def __init__(self, num_predicates, embedding_dim):
        super(LogicTensorNetwork, self).__init__()
        self.predicate_embeddings = nn.Parameter(torch.randn(num_predicates, embedding_dim))
        
    def and_op(self, x, y):
        # Product t-norm for fuzzy AND
        return x * y
    
    def or_op(self, x, y):
        # Probabilistic sum t-conorm for fuzzy OR
        return x + y - x * y
    
    def not_op(self, x):
        # Standard negation for fuzzy NOT
        return 1 - x
    
    def implies(self, x, y):
        # Standard implication for fuzzy logic
        return self.or_op(self.not_op(x), y)
    
    def forall(self, x):
        # Universal quantification as minimum
        return torch.min(x, dim=1)[0]
    
    def exists(self, x):
        # Existential quantification as maximum
        return torch.max(x, dim=1)[0]
```

### 2. Neural Theorem Provers

Neural Theorem Provers (NTPs) combine neural networks with logic programming. Here's a simplified implementation:

```python
class NeuralTheoremProver(nn.Module):
    def __init__(self, num_entities, embedding_dim):
        super(NeuralTheoremProver, self).__init__()
        self.entity_embeddings = nn.Parameter(torch.randn(num_entities, embedding_dim))
        self.unification_network = nn.Sequential(
            nn.Linear(embedding_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
    def unify(self, entity1_idx, entity2_idx):
        # Get entity embeddings
        entity1 = self.entity_embeddings[entity1_idx]
        entity2 = self.entity_embeddings[entity2_idx]
        
        # Concatenate and pass through unification network
        concat = torch.cat([entity1, entity2], dim=-1)
        unification_score = self.unification_network(concat)
        
        return unification_score
```

## Practical Example: Concept Learning with Neuro-Symbolic Models

Let's build a model that can learn visual concepts and reason about them:

```python
class ConceptLearner(nn.Module):
    def __init__(self, num_concepts=10):
        super(ConceptLearner, self).__init__()
        # Neural perception
        self.perception = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(32 * 8 * 8, 128),
            nn.ReLU()
        )
        
        # Concept detection
        self.concept_layer = nn.Linear(128, num_concepts)
        
        # Symbolic reasoning (implemented as a neural network)
        self.reasoning = nn.Sequential(
            nn.Linear(num_concepts, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 5)  # Output: 5 different high-level categories
        )
        
    def forward(self, x):
        # Neural perception
        features = self.perception(x)
        
        # Concept detection (neural to symbolic mapping)
        concepts = torch.sigmoid(self.concept_layer(features))
        
        # Symbolic reasoning
        output = self.reasoning(concepts)
        
        return output, concepts
    
    def interpret(self, concepts, threshold=0.5):
        # Convert continuous concept activations to discrete symbols
        active_concepts = (concepts > threshold).float()
        
        # Create an interpretation
        interpretations = []
        concept_names = [f"concept_{i}" for i in range(concepts.size(1))]
        
        for sample_idx in range(concepts.size(0)):
            sample_concepts = []
            for concept_idx, is_active in enumerate(active_concepts[sample_idx]):
                if is_active:
                    sample_concepts.append(concept_names[concept_idx])
            interpretations.append(sample_concepts)
            
        return interpretations
```

### Training and using the concept learner:

```python
import torch.optim as optim

# Initialize model
concept_learner = ConceptLearner(num_concepts=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(concept_learner.parameters(), lr=0.001)

# Dummy training data
x_batch = torch.randn(4, 3, 32, 32)  # 4 RGB images of size 32x32
y_batch = torch.tensor([0, 2, 1, 3])  # Target categories

# Forward pass
outputs, concepts = concept_learner(x_batch)
loss = criterion(outputs, y_batch)

# Backward pass and optimize
optimizer.zero_grad()
loss.backward()
optimizer.step()

# Interpret the concepts
interpretations = concept_learner.interpret(concepts)

print("Predictions:")
print(torch.argmax(outputs, dim=1))
print("\nActive Concepts:")
for i, interpretation in enumerate(interpretations):
    print(f"Sample {i}: {interpretation}")
```

## Summary

Neuro-symbolic models combine the strengths of neural networks and symbolic AI systems to create more powerful and interpretable artificial intelligence systems. In this tutorial, you've learned:

1. The basic principles of neuro-symbolic AI
2. How to implement neural-to-symbolic interfaces in PyTorch
3. Building differentiable logic layers
4. Creating concept learners that can detect and reason about abstract concepts
5. Applying these models to real-world problems like visual question answering

By integrating neural perception with symbolic reasoning, these models can achieve better generalization, interpretability, and data efficiency compared to pure neural networks.

## Additional Resources

- [Neurosymbolic AI: The 3rd Wave](https://arxiv.org/abs/2012.05876)
- [DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872)
- [Logic Tensor Networks](https://arxiv.org/abs/1606.04422)
- [MIT-IBM Watson AI Lab: Neuro-Symbolic AI](https://mitibmwatsonailab.mit.edu/research/neural-symbolic-ai/)

## Exercises

1. Extend the ConceptLearner to handle relational concepts (e.g., "above", "next to").
2. Implement a neuro-symbolic model for a specific domain like medical diagnosis or financial analysis.
3. Create a differentiable rule engine that can learn logical rules from data.
4. Explore ways to make the symbolic reasoning component more interpretable.
5. Implement a neuro-symbolic model that can perform multi-step reasoning.