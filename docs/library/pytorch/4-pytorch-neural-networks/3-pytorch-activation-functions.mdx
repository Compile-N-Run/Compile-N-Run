---
title: PyTorch Activation Functions
description: Learn about activation functions in PyTorch neural networks, how they work, their importance, and how to implement various activation functions in your neural network models.

---

# PyTorch Activation Functions

Activation functions are an essential component in neural networks that introduce non-linearity into the network's output. Without activation functions, neural networks would be limited to learning only linear relationships, regardless of their depth.

## What are Activation Functions?

In simple terms, an activation function decides whether a neuron should be "activated" (fired) or not based on the weighted sum of inputs and biases. This decision is what introduces non-linearity, allowing neural networks to learn complex patterns in data.

The basic flow in a neural network goes like this:

1. Input values are multiplied by weights
2. The weighted sums are calculated
3. An activation function is applied to produce the output

Let's explore the most common activation functions available in PyTorch, how they work, and when to use them.

## Common Activation Functions in PyTorch

PyTorch offers several activation functions through the `torch.nn` module. Let's examine the most widely used ones:

### 1. ReLU (Rectified Linear Unit)

ReLU is one of the most popular activation functions and is defined as:

$$f(x) = \max(0, x)$$

In PyTorch, you can implement ReLU as follows:

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# Create a ReLU activation function
relu = nn.ReLU()

# Create sample input data
x = torch.linspace(-10, 10, 100)

# Apply ReLU
y = relu(x)

# Visualize
plt.plot(x.numpy(), y.numpy())
plt.title('ReLU Activation Function')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

# Example of ReLU in a neural network layer
layer = nn.Sequential(
    nn.Linear(10, 5),
    nn.ReLU()
)
```

**Benefits of ReLU:**
- Simple and computationally efficient
- Helps mitigate the vanishing gradient problem
- Promotes sparsity in the network (many neurons can be 0)

**Drawbacks:**
- "Dying ReLU" problem: neurons can get stuck at 0 during training

### 2. Sigmoid

The sigmoid function squashes input values to a range between 0 and 1:

$$f(x) = \frac{1}{1 + e^{-x}}$$

PyTorch implementation:

```python
# Create a Sigmoid activation function
sigmoid = nn.Sigmoid()

# Create sample input data
x = torch.linspace(-10, 10, 100)

# Apply Sigmoid
y = sigmoid(x)

# Visualize
plt.plot(x.numpy(), y.numpy())
plt.title('Sigmoid Activation Function')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

# Example in a neural network
binary_classifier = nn.Sequential(
    nn.Linear(10, 5),
    nn.ReLU(),
    nn.Linear(5, 1),
    nn.Sigmoid()  # Output probability between 0 and 1
)
```

**Benefits of Sigmoid:**
- Outputs range between 0 and 1, making it useful for binary classification
- Smooth gradient

**Drawbacks:**
- Suffers from vanishing gradient problem for extreme inputs
- Output is not zero-centered

### 3. Tanh (Hyperbolic Tangent)

Tanh squashes input values to a range between -1 and 1:

$$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

PyTorch implementation:

```python
# Create a Tanh activation function
tanh = nn.Tanh()

# Create sample input data
x = torch.linspace(-10, 10, 100)

# Apply Tanh
y = tanh(x)

# Visualize
plt.plot(x.numpy(), y.numpy())
plt.title('Tanh Activation Function')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

# Example in a neural network
network = nn.Sequential(
    nn.Linear(10, 5),
    nn.Tanh(),
    nn.Linear(5, 3)
)
```

**Benefits of Tanh:**
- Zero-centered output, which helps with learning
- Similar to sigmoid but with better training dynamics

**Drawbacks:**
- Still suffers from vanishing gradient problem for extreme inputs

### 4. Leaky ReLU

Leaky ReLU addresses the "dying ReLU" problem by allowing a small negative slope:

$$f(x) = \max(0.01x, x)$$

PyTorch implementation:

```python
# Create a Leaky ReLU activation function with a negative slope of 0.01
leaky_relu = nn.LeakyReLU(0.01)

# Create sample input data
x = torch.linspace(-10, 10, 100)

# Apply Leaky ReLU
y = leaky_relu(x)

# Visualize
plt.plot(x.numpy(), y.numpy())
plt.title('Leaky ReLU Activation Function')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

# Example in a neural network
network = nn.Sequential(
    nn.Linear(10, 5),
    nn.LeakyReLU(0.01),
    nn.Linear(5, 3)
)
```

**Benefits of Leaky ReLU:**
- Prevents the "dying ReLU" problem
- Maintains most of ReLU's benefits

### 5. PReLU (Parametric ReLU)

PReLU is similar to Leaky ReLU, but the negative slope is a learnable parameter:

```python
# Create a PReLU activation function
prelu = nn.PReLU()

# In a neural network
network = nn.Sequential(
    nn.Linear(10, 5),
    nn.PReLU(),
    nn.Linear(5, 3)
)
```

### 6. ELU (Exponential Linear Unit)

ELU is similar to ReLU but has a smooth transition for negative inputs:

$$f(x) = \begin{cases}
x, & \text{if } x \geq 0 \\
\alpha(e^x - 1), & \text{if } x < 0
\end{cases}$$

PyTorch implementation:

```python
# Create an ELU activation function with alpha=1.0
elu = nn.ELU(alpha=1.0)

# Create sample input data
x = torch.linspace(-10, 10, 100)

# Apply ELU
y = elu(x)

# Visualize
plt.plot(x.numpy(), y.numpy())
plt.title('ELU Activation Function')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

# Example in a neural network
network = nn.Sequential(
    nn.Linear(10, 5),
    nn.ELU(alpha=1.0),
    nn.Linear(5, 3)
)
```

### 7. Softmax

Softmax is commonly used in the output layer of classification networks. It converts a vector of values into a probability distribution:

$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$

PyTorch implementation:

```python
# Create a Softmax activation function
softmax = nn.Softmax(dim=1)

# Create sample input data (batch of 3 samples, 5 classes)
x = torch.randn(3, 5)

# Apply softmax across the class dimension
y = softmax(x)

print("Input:")
print(x)
print("\nSoftmax output (probabilities):")
print(y)
print("\nSum of probabilities for each sample:", y.sum(dim=1))

# Example in a neural network for multi-class classification
classifier = nn.Sequential(
    nn.Linear(10, 128),
    nn.ReLU(),
    nn.Linear(128, 5),
    nn.Softmax(dim=1)  # Convert to probabilities across 5 classes
)
```

**Output might look like:**
```
Input:
tensor([[ 0.5274,  0.9800, -0.4137,  1.0731, -0.6177],
        [ 0.3240, -0.2863,  1.1857, -0.1955, -0.5822],
        [-0.1381,  0.5877,  0.5373,  0.8660,  1.0606]])

Softmax output (probabilities):
tensor([[0.1921, 0.3019, 0.0751, 0.3312, 0.0997],
        [0.2038, 0.1114, 0.4828, 0.1217, 0.0804],
        [0.1033, 0.2139, 0.2040, 0.2826, 0.2962]])

Sum of probabilities for each sample: tensor([1., 1., 1.])
```

## Using Activation Functions in Neural Networks

There are two main ways to include activation functions in your PyTorch neural networks:

### 1. As part of `nn.Sequential`

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 10),
    nn.Softmax(dim=1)
)
```

### 2. In a custom module

```python
import torch
import torch.nn as nn

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.softmax(self.fc3(x))
        return x
```

### 3. Using functional API

PyTorch also provides a functional interface for activation functions:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.softmax(self.fc3(x), dim=1)
        return x
```

## Practical Example: Image Classification with MNIST

Let's build a simple neural network for classifying MNIST digits, showing how activation functions fit into a real network:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Download and load the MNIST dataset
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

# Define a neural network with different activation functions
class MNISTClassifier(nn.Module):
    def __init__(self, activation_function=nn.ReLU()):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.activation = activation_function
        
    def forward(self, x):
        x = self.flatten(x)
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)  # No activation on the final layer (will use CrossEntropyLoss)
        return x

# Create models with different activation functions
relu_model = MNISTClassifier(activation_function=nn.ReLU())
tanh_model = MNISTClassifier(activation_function=nn.Tanh())
leaky_model = MNISTClassifier(activation_function=nn.LeakyReLU(0.01))

# Function to train a model
def train_model(model, epochs=3):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss()
    
    for epoch in range(epochs):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)
            loss.backward()
            optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')
    
    # Test the model
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
    
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'Test accuracy: {accuracy:.2f}%')
    return accuracy

# Compare different activation functions
print("Training model with ReLU activation:")
relu_accuracy = train_model(relu_model)

print("\nTraining model with Tanh activation:")
tanh_accuracy = train_model(tanh_model)

print("\nTraining model with LeakyReLU activation:")
leaky_accuracy = train_model(leaky_model)

print("\nAccuracy comparison:")
print(f"ReLU: {relu_accuracy:.2f}%")
print(f"Tanh: {tanh_accuracy:.2f}%")
print(f"LeakyReLU: {leaky_accuracy:.2f}%")
```

This example trains three models using different activation functions and compares their performance on the MNIST dataset.

## How to Choose the Right Activation Function

Selecting the appropriate activation function depends on the specific characteristics of your task:

1. **ReLU** is a good default choice for hidden layers in most neural networks
2. **LeakyReLU** or **PReLU** can be used when you suspect "dying ReLU" might be an issue
3. **Tanh** can be useful for bounded outputs or in specific architectures like RNNs
4. **Sigmoid** is typically used for binary classification output layers
5. **Softmax** is used for multi-class classification output layers

## Visualizing Activation Functions

Let's create a comprehensive visualization of different activation functions:

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Create a range of input values
x = torch.linspace(-10, 10, 1000)

# Define activation functions
activations = {
    'ReLU': nn.ReLU(),
    'Leaky ReLU (0.1)': nn.LeakyReLU(0.1),
    'Sigmoid': nn.Sigmoid(),
    'Tanh': nn.Tanh(),
    'ELU': nn.ELU(),
    'SELU': nn.SELU(),
}

# Create a plot
plt.figure(figsize=(12, 8))

# Plot each activation function
for name, activation in activations.items():
    y = activation(x).numpy()
    plt.plot(x.numpy(), y, label=name, linewidth=2)

# Add grid and legends
plt.grid(True)
plt.legend(loc='best', fontsize=12)
plt.title('Comparison of Activation Functions', fontsize=16)
plt.xlabel('Input', fontsize=14)
plt.ylabel('Output', fontsize=14)
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
plt.xlim(-5, 5)
plt.tight_layout()
plt.show()
```

## Summary

Activation functions are crucial elements in neural networks that introduce non-linearity, enabling the network to learn complex patterns. We've covered:

- The purpose and importance of activation functions
- Common activation functions in PyTorch (ReLU, Leaky ReLU, Sigmoid, Tanh, ELU, Softmax)
- How to implement these functions in neural network architectures
- A practical example showing how different activation functions perform on MNIST
- Guidelines for choosing the right activation function for your task

By understanding and appropriately using activation functions, you can significantly improve the performance and training dynamics of your neural networks.

## Further Exercises and Resources

### Exercises:

1. Implement a custom activation function that combines properties of two existing functions.
2. Experiment with different activation functions on the CIFAR-10 dataset and compare their performance.
3. Visualize the gradients of different activation functions and analyze how they might affect training.
4. Create a neural network that uses different activation functions in different layers and analyze its performance.

### Additional Resources:

- [PyTorch Documentation on nn.Module](https://pytorch.org/docs/stable/nn.html)
- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
- [Deep Learning Book by Ian Goodfellow et al.](https://www.deeplearningbook.org/) - Chapter 6 covers activation functions in detail
- [CS231n Stanford Course: Activation Functions](http://cs231n.github.io/neural-networks-1/#actfun)

With these tools and knowledge, you're now better equipped to choose and implement the right activation functions for your PyTorch neural networks!