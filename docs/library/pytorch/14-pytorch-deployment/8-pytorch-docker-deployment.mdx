---
title: PyTorch Docker Deployment
description: Learn how to containerize and deploy PyTorch models using Docker for consistent, scalable, and reproducible machine learning deployments

---

# PyTorch Docker Deployment

## Introduction

Deploying PyTorch models into production environments can be challenging due to dependency management, environment consistency, and scalability requirements. Docker provides an elegant solution to these challenges by allowing you to package your PyTorch model along with all its dependencies into a standardized container. This container can then be deployed consistently across different environments, from development to production.

In this guide, we'll explore how to containerize PyTorch models using Docker, create efficient deployment workflows, and implement best practices for production-ready model serving.

## Why Use Docker for PyTorch Deployment?

Before diving into implementation, let's understand the benefits of using Docker for PyTorch deployment:

1. **Environment Consistency** - Eliminate "it works on my machine" problems by packaging your model with its exact environment
2. **Isolation** - Run your model without interference from other applications
3. **Scalability** - Easily scale horizontally by deploying multiple containers
4. **Versioning** - Track different versions of your model and its environment
5. **Portability** - Deploy the same container to any platform that supports Docker

## Prerequisites

To follow along with this guide, you'll need:

- Basic knowledge of PyTorch
- Docker installed on your system
- A PyTorch model ready for deployment

## Creating a Basic PyTorch Docker Image

Let's start by creating a simple Docker image for a PyTorch application.

### Step 1: Create a PyTorch Model

First, let's create a simple PyTorch model that we'll containerize. Save this as `model.py`:

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 1)
        
    def forward(self, x):
        return torch.sigmoid(self.fc(x))

# Create and save the model
model = SimpleModel()
dummy_input = torch.randn(10)
traced_model = torch.jit.trace(model, dummy_input)
torch.jit.save(traced_model, "model.pt")
```

### Step 2: Create an Inference Script

Next, let's create a simple inference script that loads the model and makes predictions. Save this as `inference.py`:

```python
import torch
import json
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load the model
model = torch.jit.load("model.pt")
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    input_tensor = torch.tensor(data['input'], dtype=torch.float32)
    
    with torch.no_grad():
        output = model(input_tensor)
    
    return jsonify({
        'prediction': output.item()
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### Step 3: Create a Requirements File

Create a `requirements.txt` file listing all the dependencies:

```
torch==2.0.1
flask==2.3.2
```

### Step 4: Create a Dockerfile

Now, let's create a Dockerfile that will define our container:

```dockerfile
# Start from PyTorch official image
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

# Set working directory
WORKDIR /app

# Copy requirements first (for better caching)
COPY requirements.txt .

# Install dependencies
RUN pip install -r requirements.txt

# Copy the model and inference script
COPY model.py .
COPY inference.py .

# Generate the model file
RUN python model.py

# Expose the port
EXPOSE 5000

# Run the inference server when the container launches
CMD ["python", "inference.py"]
```

### Step 5: Build the Docker Image

Now, let's build the Docker image:

```bash
docker build -t pytorch-model-serving .
```

This will create a Docker image named `pytorch-model-serving` based on the instructions in the Dockerfile.

### Step 6: Run the Docker Container

Once the image is built, we can run it:

```bash
docker run -p 5000:5000 pytorch-model-serving
```

This command starts the container and maps port 5000 from the container to port 5000 on your host machine.

### Step 7: Test the API

With the container running, you can now make predictions by sending requests to the API:

```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"input": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}'
```

You should receive a response like:

```json
{"prediction": 0.7456598877906799}
```

## Advanced Docker Techniques for PyTorch Deployment

Now that we have a basic deployment working, let's explore some advanced techniques to improve our Docker deployment.

### Multi-Stage Builds for Smaller Images

Multi-stage builds allow you to create smaller, more efficient Docker images by separating the build environment from the runtime environment:

```dockerfile
# Build stage
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime as builder

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY model.py .
RUN python model.py

# Runtime stage
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY inference.py .
COPY --from=builder /app/model.pt .
EXPOSE 5000
CMD ["python", "inference.py"]
```

### Using Docker Compose for Multi-Container Applications

For more complex deployments involving multiple services (like a database, cache, etc.), Docker Compose is a great tool:

Create a `docker-compose.yml` file:

```yaml
version: '3'
services:
  model-service:
    build: .
    ports:
      - "5000:5000"
    restart: always
    environment:
      - MODEL_PATH=/app/model.pt
      - LOG_LEVEL=INFO
  
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

Start the services with:

```bash
docker-compose up
```

### GPU Support

When deploying models that benefit from GPU acceleration, you need to enable GPU access in your Docker container:

```dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

# ... rest of Dockerfile

# Update the inference script to use CUDA if available
COPY inference_gpu.py ./inference.py
```

And run with GPU support:

```bash
docker run --gpus all -p 5000:5000 pytorch-model-serving
```

Here's an updated `inference_gpu.py` that uses CUDA if available:

```python
import torch
import json
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = torch.jit.load("model.pt")
model.to(device)
model.eval()

print(f"Using device: {device}")

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    input_tensor = torch.tensor(data['input'], dtype=torch.float32).to(device)
    
    with torch.no_grad():
        output = model(input_tensor)
    
    return jsonify({
        'prediction': output.item(),
        'device': str(device)
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## Best Practices for Production Deployment

When deploying PyTorch models to production using Docker, consider the following best practices:

### 1. Optimize Image Size

Large Docker images can slow down deployment and waste resources:

- Use multi-stage builds (as shown above)
- Remove unnecessary packages and files
- Use `.dockerignore` to exclude unnecessary files from the build context

Example `.dockerignore` file:

```
__pycache__
*.pyc
.git
.pytest_cache
notebooks/
tests/
```

### 2. Health Checks

Add health checks to your Docker container to ensure the service is working correctly:

```dockerfile
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:5000/health || exit 1
```

And add a health endpoint to your Flask app:

```python
@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy'})
```

### 3. Proper Logging

Configure proper logging in your application for monitoring and debugging:

```python
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

@app.route('/predict', methods=['POST'])
def predict():
    logging.info(f"Received prediction request")
    # Rest of the function
```

### 4. Environment Variables for Configuration

Use environment variables to configure your application:

```python
import os

# Get configuration from environment variables
model_path = os.environ.get('MODEL_PATH', 'model.pt')
log_level = os.environ.get('LOG_LEVEL', 'INFO')

# Configure logging based on environment variable
logging.basicConfig(level=getattr(logging, log_level))
```

### 5. Version Your Models

Include versioning information in your API responses:

```python
MODEL_VERSION = "1.0.0"

@app.route('/predict', methods=['POST'])
def predict():
    # ... prediction logic
    return jsonify({
        'prediction': output.item(),
        'model_version': MODEL_VERSION
    })
```

## Real-World Examples

Let's look at a more complete real-world example that includes model monitoring and parallel processing for handling multiple requests.

### Example: Scalable Image Classification Service

```python
# app.py
import os
import time
import torch
import torchvision.transforms as transforms
from PIL import Image
from flask import Flask, request, jsonify
from torchvision.models import resnet18
import io
import logging
from concurrent.futures import ThreadPoolExecutor

app = Flask(__name__)
MODEL_VERSION = "1.0.0"

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.environ.get('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = resnet18(pretrained=True)
model.to(device)
model.eval()

# Load ImageNet class names
with open('imagenet_classes.txt') as f:
    classes = [line.strip() for line in f.readlines()]

# Initialize the transforms for input images
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Create a thread pool for parallel processing
executor = ThreadPoolExecutor(max_workers=4)

# Track metrics
request_count = 0
start_time = time.time()

def process_image(image_bytes):
    image = Image.open(io.BytesIO(image_bytes))
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(image_tensor)
        _, predicted = outputs.max(1)
        category_idx = predicted.item()
    
    return {
        'class': classes[category_idx],
        'class_id': category_idx
    }

@app.route('/predict', methods=['POST'])
def predict():
    global request_count
    request_count += 1
    
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    image_bytes = file.read()
    
    try:
        # Submit the job to the thread pool
        future = executor.submit(process_image, image_bytes)
        result = future.result()
        
        # Add metadata to the response
        result['model_version'] = MODEL_VERSION
        result['device'] = str(device)
        
        logging.info(f"Processed image, predicted class: {result['class']}")
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"Error processing image: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    # More thorough health check
    uptime = time.time() - start_time
    
    # Try a test prediction to ensure model is working
    try:
        dummy_input = torch.randn(1, 3, 224, 224).to(device)
        with torch.no_grad():
            model(dummy_input)
        model_healthy = True
    except Exception:
        model_healthy = False
    
    return jsonify({
        'status': 'healthy' if model_healthy else 'unhealthy',
        'uptime': uptime,
        'request_count': request_count,
        'model_version': MODEL_VERSION
    })

@app.route('/metrics', methods=['GET'])
def metrics():
    uptime = time.time() - start_time
    return jsonify({
        'uptime': uptime,
        'request_count': request_count,
        'requests_per_second': request_count / uptime if uptime > 0 else 0
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    logging.info(f"Starting server on port {port}")
    logging.info(f"Using device: {device}")
    app.run(host='0.0.0.0', port=port)
```

The corresponding Dockerfile:

```dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

WORKDIR /app

# Copy requirements first (for better caching)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download ImageNet classes
RUN wget -O imagenet_classes.txt \
    https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt

# Copy application code
COPY app.py .

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:5000/health || exit 1

# Set environment variables
ENV MODEL_VERSION=1.0.0
ENV LOG_LEVEL=INFO
ENV PORT=5000

# Expose the port
EXPOSE 5000

# Run the inference server when the container launches
CMD ["python", "app.py"]
```

To deploy this service:

1. Build the Docker image:
   ```bash
   docker build -t pytorch-image-classifier .
   ```

2. Run the container:
   ```bash
   docker run -p 5000:5000 -e LOG_LEVEL=INFO pytorch-image-classifier
   ```

3. Test the prediction endpoint:
   ```bash
   curl -X POST -F "file=@cat.jpg" http://localhost:5000/predict
   ```

## Orchestrating with Kubernetes

For production deployments, you might want to use Kubernetes for orchestration. Here's a simple Kubernetes deployment manifest:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pytorch-model
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pytorch-model
  template:
    metadata:
      labels:
        app: pytorch-model
    spec:
      containers:
      - name: model-container
        image: pytorch-image-classifier:latest
        ports:
        - containerPort: 5000
        resources:
          limits:
            memory: "2Gi"
            cpu: "1"
          requests:
            memory: "1Gi"
            cpu: "0.5"
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 30
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 60
---
apiVersion: v1
kind: Service
metadata:
  name: pytorch-model-service
spec:
  selector:
    app: pytorch-model
  ports:
  - port: 80
    targetPort: 5000
  type: LoadBalancer
```

## Summary

In this guide, we've covered the fundamentals of deploying PyTorch models using Docker. From creating a basic Docker image, to implementing best practices for production deployments, and even integrating with Kubernetes for orchestration. Here's what we learned:

1. How to create a Docker image for a PyTorch model
2. How to optimize Docker images using multi-stage builds
3. How to add health checks and proper monitoring
4. How to scale PyTorch deployments using Docker Compose and Kubernetes
5. Best practices for production-ready deployments

By containerizing your PyTorch models with Docker, you can ensure consistent, reproducible, and scalable deployments across different environments and platforms.

## Additional Resources

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Docker Documentation](https://docs.docker.com/)
- [TorchServe](https://pytorch.org/serve/) - A model serving library for PyTorch
- [NVIDIA Container Runtime](https://github.com/NVIDIA/nvidia-container-runtime) - For GPU support in containers

## Exercises

1. **Basic Deployment**: Modify the Dockerfile to use a different PyTorch version.
2. **Optimization**: Implement a multi-stage build to reduce the size of the Docker image.
3. **Advanced Features**: Add a profiling endpoint to your Flask app that shows the execution time for predictions.
4. **Performance**: Implement batch processing of requests to improve throughput.
5. **Monitoring**: Integrate Prometheus metrics to track model performance and accuracy over time.