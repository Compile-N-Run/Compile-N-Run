---
title: PyTorch Edge Deployment
description: Learn how to deploy PyTorch models to edge devices like mobile phones, IoT devices, and embedded systems for efficient on-device inference.

---

# PyTorch Edge Deployment

## Introduction

Edge deployment refers to running machine learning models directly on end-user devices rather than in the cloud. These "edge devices" include smartphones, IoT sensors, embedded systems, and other hardware with limited computational resources. PyTorch offers several tools and techniques to optimize and deploy models to such resource-constrained environments.

In this guide, we'll explore how to take your trained PyTorch models and deploy them efficiently to edge devices. We'll cover optimization techniques, conversion tools, and practical workflows to make your models run smoothly on devices with limited memory, processing power, and energy capacity.

## Why Deploy to the Edge?

Before diving into the technical details, let's understand why edge deployment matters:

- **Reduced latency**: No need to send data to the cloud and wait for results
- **Offline functionality**: Models work even without internet connectivity
- **Privacy preservation**: Sensitive data stays on the user's device
- **Lower bandwidth costs**: No constant data transfer to cloud servers
- **Real-time applications**: Enables use cases requiring immediate inference

## Prerequisites

To follow along with this guide, you should have:

- Basic understanding of PyTorch
- A trained PyTorch model ready for deployment
- Python development environment
- Familiarity with basic terminal commands

## Preparing Your PyTorch Model for Edge Deployment

### Step 1: Model Optimization

Before deploying to edge devices, we need to optimize our model to reduce its size and computational requirements.

#### Model Pruning

Pruning removes unnecessary connections in your neural network:

```python
import torch
from torch.nn.utils import prune

# Load your model
model = torch.load('my_model.pth')

# Prune 30% of the least important weights in a layer
prune.l1_unstructured(model.conv1, name="weight", amount=0.3)

# Make the pruning permanent
prune.remove(model.conv1, 'weight')

# Save the pruned model
torch.save(model, 'my_model_pruned.pth')
```

#### Quantization

Quantization reduces the precision of the weights from 32-bit float to 8-bit integers:

```python
import torch

# Load your model
model = torch.load('my_model.pth')
model.eval()

# Quantize the model to 8-bit integers
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Save the quantized model
torch.save(quantized_model, 'my_model_quantized.pth')
```

### Step 2: Model Format Conversion

Different edge platforms require different model formats. Here's how to convert your PyTorch model to some common ones:

#### ONNX Format

ONNX (Open Neural Network Exchange) is a standard format that's supported by many edge frameworks:

```python
import torch
import torchvision

# Load your model
model = torch.load('my_model.pth')
model.eval()

# Create a dummy input with the expected size
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,               # model being run
    dummy_input,         # model input
    "model.onnx",        # output file
    export_params=True,  # store the trained parameter weights
    opset_version=11,    # ONNX version
    do_constant_folding=True,  # optimization
    input_names=['input'],     # input names
    output_names=['output'],   # output names
    dynamic_axes={'input': {0: 'batch_size'},  # variable length axes
                  'output': {0: 'batch_size'}}
)

print("Model exported to ONNX format")
```

#### TorchScript Format

TorchScript is PyTorch's own optimized format:

```python
import torch

# Load your model
model = torch.load('my_model.pth')
model.eval()

# Convert to TorchScript via tracing
example_input = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.trace(model, example_input)

# Save the TorchScript model
traced_script_module.save("model_torchscript.pt")
print("Model exported to TorchScript format")
```

## Deploying to Different Edge Platforms

### Mobile Deployment with PyTorch Mobile

PyTorch Mobile allows you to run PyTorch models on iOS and Android devices.

#### Export for Mobile

```python
import torch

# Load your model
model = torch.load('my_model.pth')
model.eval()

# Convert to TorchScript
example_input = torch.rand(1, 3, 224, 224)
script_model = torch.jit.trace(model, example_input)

# Optimize for mobile
optimized_model = torch.utils.mobile_optimizer.optimize_for_mobile(script_model)

# Save the model for mobile
optimized_model.save("model_mobile.pt")
print("Model optimized and exported for mobile")
```

#### Android Integration

Here's how to use your model in an Android application:

```java
// In your Android app's build.gradle file
dependencies {
    implementation 'org.pytorch:pytorch_android:1.10.0'
    implementation 'org.pytorch:pytorch_android_torchvision:1.10.0'
}
```

```java
// In your Android activity or fragment
import org.pytorch.IValue;
import org.pytorch.Module;
import org.pytorch.Tensor;
import org.pytorch.torchvision.TensorImageUtils;

// Load the model
Module module = Module.load(assetFilePath(this, "model_mobile.pt"));

// Prepare input tensor
Bitmap bitmap = /* your input image */;
final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(
    bitmap,
    new float[] {0.485f, 0.456f, 0.406f},
    new float[] {0.229f, 0.224f, 0.225f}
);

// Run inference
final Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();
final float[] scores = outputTensor.getDataAsFloatArray();

// Process results
```

#### iOS Integration

For iOS apps using Swift:

```swift
// In your Podfile
target 'YourApp' do
  pod 'LibTorch', '~> 1.10.0'
end
```

```swift
// In your Swift code
import LibTorch

// Load the model
let filePath = Bundle.main.path(forResource: "model_mobile", ofType: "pt")!
let module = try! TorchModule(fileAtPath: filePath)

// Prepare input tensor
let image = /* your input image */
let tensorWidth = 224
let tensorHeight = 224
var tensor = Tensor(shape: [1, 3, Int(tensorHeight), Int(tensorWidth)], dtype: .float)

// Convert image to tensor...

// Run inference
let output = try! module.forward([tensor]).get(at: 0)

// Process results
let results = output.data.toFloatArray()
```

### Embedded Systems Deployment

For deployment on embedded systems like Raspberry Pi or NVIDIA Jetson:

#### Setup on Raspberry Pi

```bash
# Install dependencies
sudo apt-get update
sudo apt-get install -y python3-pip
pip3 install torch torchvision torchaudio

# For better performance with specific hardware
pip3 install onnxruntime
```

#### Inference Code

```python
import torch
import time

# Load the model (TorchScript or regular PyTorch model)
model = torch.jit.load('model_torchscript.pt')
model.eval()

# Disable gradient calculations for inference
with torch.no_grad():
    # Prepare input (e.g., from a camera or sensor)
    input_tensor = torch.rand(1, 3, 224, 224)
    
    # Measure inference time
    start_time = time.time()
    output = model(input_tensor)
    inference_time = time.time() - start_time
    
    print(f"Inference completed in {inference_time*1000:.2f}ms")
    print(f"Output shape: {output.shape}")
    
    # Process the output as needed for your application
    # ...
```

## Advanced Optimization Techniques

### TensorRT Integration

NVIDIA TensorRT can drastically speed up inference on NVIDIA GPUs:

```python
import torch
import torch_tensorrt

# Load your PyTorch model
model = torch.load('my_model.pth')
model.eval().cuda()

# Convert to TensorRT
trt_model = torch_tensorrt.compile(
    model,
    inputs=[torch_tensorrt.Input(
        min_shape=[1, 3, 224, 224],
        opt_shape=[1, 3, 224, 224],
        max_shape=[1, 3, 224, 224],
        dtype=torch.float32
    )],
    enabled_precisions={torch.float, torch.half}  # Allow FP32 and FP16
)

# Save the TensorRT model
torch.jit.save(trt_model, "model_tensorrt.pt")

# Inference with the TensorRT model
input_tensor = torch.randn(1, 3, 224, 224).cuda()
output = trt_model(input_tensor)
```

### Quantization-Aware Training

For better accuracy in quantized models:

```python
import torch
from torch.quantization import get_default_qconfig, prepare_qat, convert

# Define a model
model = YourModel()

# Set the model to training mode
model.train()

# Specify quantization configuration
model.qconfig = get_default_qconfig('fbgemm')

# Prepare model for QAT
model_prepared = prepare_qat(model)

# Train the model (with your typical training loop)
# ...

# Convert to quantized model
model_prepared.eval()
model_quantized = convert(model_prepared)

# Save the quantized model
torch.save(model_quantized.state_dict(), 'model_quantized.pth')
```

## Real-World Application Example

Let's implement a complete example of deploying a PyTorch image classification model to a Raspberry Pi for real-time inference:

### Step 1: Prepare the Model

```python
import torch
import torchvision

# Load a pre-trained MobileNetV2 model
model = torchvision.models.mobilenet_v2(pretrained=True)
model.eval()

# Quantize the model
model_quantized = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8
)

# Export to TorchScript
example_input = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.trace(model_quantized, example_input)
traced_script_module.save("mobilenet_v2_quantized.pt")
```

### Step 2: Deploy to Raspberry Pi

```python
import torch
import torchvision.transforms as transforms
from PIL import Image
import time

# Load the model
model = torch.jit.load('mobilenet_v2_quantized.pt')
model.eval()

# Prepare image preprocessing
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load class labels
with open('imagenet_classes.txt') as f:
    labels = [line.strip() for line in f.readlines()]

def classify_image(image_path):
    # Load and preprocess the image
    img = Image.open(image_path)
    input_tensor = preprocess(img)
    input_batch = input_tensor.unsqueeze(0)
    
    # Run inference
    with torch.no_grad():
        start_time = time.time()
        output = model(input_batch)
        inference_time = time.time() - start_time
    
    # Process results
    probabilities = torch.nn.functional.softmax(output[0], dim=0)
    top5_prob, top5_indices = torch.topk(probabilities, 5)
    
    # Print results
    print(f"Inference time: {inference_time*1000:.2f}ms")
    print("Top 5 predictions:")
    for i in range(5):
        print(f"{labels[top5_indices[i]]}: {top5_prob[i].item()*100:.2f}%")

# Example usage
classify_image('cat.jpg')
```

### Step 3: Create a Simple Live Camera Classification Script

```python
import torch
import torchvision.transforms as transforms
from PIL import Image
import time
import cv2

# Load the model
model = torch.jit.load('mobilenet_v2_quantized.pt')
model.eval()

# Prepare image preprocessing
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load class labels
with open('imagenet_classes.txt') as f:
    labels = [line.strip() for line in f.readlines()]

# Start video capture
cap = cv2.VideoCapture(0)

while True:
    # Capture frame
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert to PIL Image
    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    
    # Preprocess
    input_tensor = preprocess(img)
    input_batch = input_tensor.unsqueeze(0)
    
    # Run inference
    with torch.no_grad():
        start_time = time.time()
        output = model(input_batch)
        inference_time = time.time() - start_time
    
    # Process results
    probabilities = torch.nn.functional.softmax(output[0], dim=0)
    top_prob, top_idx = torch.topk(probabilities, 1)
    label = labels[top_idx.item()]
    
    # Display results on frame
    cv2.putText(frame, f"{label}: {top_prob.item()*100:.1f}%", 
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    cv2.putText(frame, f"Inference: {inference_time*1000:.1f}ms", 
                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    
    # Show frame
    cv2.imshow('Live Classification', frame)
    
    # Exit on 'q' press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
```

## Performance Monitoring and Optimization

When deploying to the edge, it's important to monitor key performance metrics:

```python
import time
import psutil
import torch

# Load your model
model = torch.jit.load('my_model_optimized.pt')
model.eval()

# Prepare input
input_tensor = torch.rand(1, 3, 224, 224)

# Get baseline memory usage
process = psutil.Process()
memory_before = process.memory_info().rss / 1024 / 1024  # in MB

# Warm-up run
_ = model(input_tensor)

# Measure inference time (average of 10 runs)
total_time = 0
runs = 10

for _ in range(runs):
    start_time = time.time()
    output = model(input_tensor)
    total_time += time.time() - start_time

avg_time = total_time / runs
memory_after = process.memory_info().rss / 1024 / 1024  # in MB
memory_used = memory_after - memory_before

print(f"Average inference time: {avg_time*1000:.2f}ms")
print(f"Memory usage: {memory_used:.2f}MB")
```

## Troubleshooting Common Issues

### Memory Limitations

If your model is too large for your device's memory:
- Try further quantization
- Prune more aggressively
- Split your model into smaller components
- Use operator fusion to reduce memory overhead

### Slow Inference

If inference is too slow:
- Use a lighter model architecture
- Apply more aggressive quantization
- Ensure you're using hardware acceleration if available
- Consider using model distillation to create a smaller, faster model

### Accuracy Degradation

If your optimized model has reduced accuracy:
- Try quantization-aware training instead of post-training quantization
- Use a higher bit-width for quantization (e.g., INT16 instead of INT8)
- Apply pruning more conservatively
- Fine-tune the model after optimization

## Summary

In this guide, we've explored the key aspects of deploying PyTorch models to edge devices:

1. **Model Optimization** techniques like pruning and quantization to reduce size and improve performance
2. **Format Conversion** to TorchScript, ONNX, and other formats suitable for edge devices
3. **Platform-Specific Deployment** for mobile (Android and iOS) and embedded systems
4. **Advanced Optimization** with TensorRT and quantization-aware training
5. **Real-World Applications** with complete examples of image classification on edge devices

Edge deployment enables exciting use cases like real-time computer vision, on-device natural language processing, and intelligent IoT applications - all while preserving user privacy and enabling offline functionality.

## Additional Resources

- [PyTorch Mobile Documentation](https://pytorch.org/mobile/home/)
- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [TensorRT Documentation](https://developer.nvidia.com/tensorrt)
- [Quantization in PyTorch](https://pytorch.org/docs/stable/quantization.html)
- [Model Optimization Techniques](https://pytorch.org/tutorials/recipes/recipes/model_preparation_for_deployment.html)

## Exercises

1. Take a pre-trained PyTorch model (like ResNet18) and quantize it to 8-bit precision.
2. Convert a PyTorch model to ONNX format and validate the conversion using onnx.checker.
3. Write a simple Android app that uses PyTorch Mobile to classify images from the camera.
4. Deploy a PyTorch model to a Raspberry Pi and measure its inference performance.
5. Experiment with different quantization techniques and compare the trade-offs between model size, speed, and accuracy.

Happy deploying!