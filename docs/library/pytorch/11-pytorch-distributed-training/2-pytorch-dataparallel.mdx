---
title: "PyTorch DataParallel"
description: "Learn how to use PyTorch's DataParallel to train deep learning models across multiple GPUs for faster training and processing of larger batch sizes."

---

# PyTorch DataParallel

## Introduction

Training deep learning models can be computationally intensive and time-consuming. As models get more complex and datasets grow larger, leveraging multiple GPUs becomes essential to speed up the training process. PyTorch's `DataParallel` is a simple yet powerful tool that enables data parallelism across multiple GPUs on a single machine.

In this tutorial, we'll explore:
- What `DataParallel` is and how it works
- When to use `DataParallel` vs. other distributed training options
- How to implement `DataParallel` in your PyTorch code
- Best practices and common pitfalls

## What is DataParallel?

`DataParallel` is a PyTorch wrapper that enables parallel processing across multiple GPUs by:

1. Splitting the input data batch across available GPUs
2. Replicating the model on each GPU
3. Processing different slices of data in parallel
4. Gathering and combining the results

This approach is called **data parallelism**, where the same model is replicated across devices, but each processes different data samples.

## When to Use DataParallel

`DataParallel` is ideal when:
- You have a single machine with multiple GPUs
- Your model fits in the memory of a single GPU
- You want a simple implementation without complex distributed setup
- You're looking to increase your batch size or reduce training time

## Basic Implementation

Let's start with a simple example of how to use `DataParallel`:

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(1000, 100),
            nn.ReLU(),
            nn.Linear(100, 10)
        )
    
    def forward(self, x):
        return self.layer(x)

# Create model instance
model = SimpleModel()

# Check if CUDA is available
if torch.cuda.is_available():
    # Wrap model with DataParallel
    model = nn.DataParallel(model)
    
    # Move model to GPU
    model = model.cuda()
    
    print(f"Training on {torch.cuda.device_count()} GPUs")
else:
    print("CUDA is not available. Training on CPU")

# Now use the model as usual
# DataParallel takes care of distributing the input and gathering the outputs
```

When you run this code, if you have multiple GPUs, you'll see output like:

```
Training on 4 GPUs
```

## How DataParallel Works

Let's break down what happens when you use `DataParallel`:

1. **Model Replication**: The model is replicated (copied) to all available GPUs.

2. **Data Splitting**: When you pass a batch of data to the model, `DataParallel` automatically splits it across the available GPUs.

3. **Forward Pass**: Each GPU performs the forward pass on its portion of the data independently.

4. **Result Gathering**: The outputs from all GPUs are gathered and returned as a single tensor.

5. **Backward Pass**: During backward pass, gradients are computed on each GPU and automatically synchronized.

## Complete Training Example

Let's implement a complete training loop using `DataParallel`:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import time

# Create synthetic dataset
def create_dataset(size=10000, dims=1000):
    X = torch.randn(size, dims)
    y = torch.randint(0, 10, (size,))
    return TensorDataset(X, y)

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1000, 500),
            nn.ReLU(),
            nn.Linear(500, 200),
            nn.ReLU(),
            nn.Linear(200, 10)
        )
    
    def forward(self, x):
        return self.network(x)

# Create dataset and dataloader
train_dataset = create_dataset()
train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)

# Initialize model
model = NeuralNetwork()

# Check for GPU availability
if torch.cuda.is_available():
    device = torch.device("cuda")
    # Wrap model with DataParallel if multiple GPUs are available
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")
        model = nn.DataParallel(model)
    model = model.to(device)
else:
    device = torch.device("cpu")
    print("Using CPU")

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 5
start_time = time.time()

for epoch in range(num_epochs):
    running_loss = 0.0
    
    for i, (inputs, labels) in enumerate(train_loader):
        # Move data to device
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Zero the parameter gradients
        optimizer.zero_grad()
        
        # Forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        # Update statistics
        running_loss += loss.item()
        
        if i % 10 == 9:
            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/10:.4f}')
            running_loss = 0.0

elapsed_time = time.time() - start_time
print(f"Training completed in {elapsed_time:.2f} seconds")
```

Sample output (with 4 GPUs):

```
Using 4 GPUs!
Epoch 1, Batch 10, Loss: 2.3058
Epoch 1, Batch 20, Loss: 2.2814
...
Epoch 5, Batch 190, Loss: 0.1245
Training completed in 45.23 seconds
```

## Performance Considerations

Using `DataParallel` doesn't always guarantee faster training. Here are some considerations:

### Batch Size

With `DataParallel`, your effective batch size increases by the number of GPUs. For example, if your batch size is 64 and you're using 4 GPUs, each GPU processes 16 samples, but the effective batch size is still 64.

If you want to take full advantage of multiple GPUs, consider increasing your batch size:

```python
# If you have 4 GPUs and want each to process a batch of 64
batch_size = 64 * torch.cuda.device_count()  # 256 with 4 GPUs
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
```

### GPU Utilization

To check GPU utilization during training:

```python
def print_gpu_utilization():
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"Memory Allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB")
            print(f"Memory Reserved: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB")

# Call this function during training
print_gpu_utilization()
```

## Real-World Example: ResNet Training

Let's use `DataParallel` to train a ResNet model on the CIFAR-10 dataset:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

# Define transforms for the training data
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, shuffle=True, num_workers=2)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load a pre-defined ResNet model
model = torchvision.models.resnet18(pretrained=False)
# Modify the first layer to work with CIFAR-10
model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
# Modify the final layer for 10 classes
model.fc = nn.Linear(model.fc.in_features, 10)

# Use DataParallel
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)

model = model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# Training loop
num_epochs = 5
start_time = time.time()

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        if i % 50 == 49:
            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {running_loss/50:.3f}, '
                  f'Accuracy: {100.*correct/total:.2f}%')
            running_loss = 0.0
    
    # Update learning rate
    scheduler.step()

elapsed_time = time.time() - start_time
print(f"Training completed in {elapsed_time:.2f} seconds")
print(f"Final accuracy: {100.*correct/total:.2f}%")
```

Sample output:

```
Using 4 GPUs
Epoch: 1, Batch: 50, Loss: 1.723, Accuracy: 37.51%
Epoch: 1, Batch: 100, Loss: 1.542, Accuracy: 43.27%
...
Epoch: 5, Batch: 350, Loss: 0.512, Accuracy: 82.45%
Epoch: 5, Batch: 400, Loss: 0.498, Accuracy: 83.11%
Training completed in 325.67 seconds
Final accuracy: 83.21%
```

## Common Issues and Solutions

### 1. Uneven Batch Sizes

If your batch size isn't divisible by the number of GPUs, the last GPU might receive fewer samples. This can lead to errors with batch normalization. To avoid this:

```python
# Make sure batch size is divisible by number of GPUs
num_gpus = torch.cuda.device_count()
batch_size = 128  # Base batch size
batch_size = (batch_size // num_gpus) * num_gpus  # Ensure it's divisible
```

### 2. Model on CPU after Loading from Checkpoint

When loading a model that was saved with `DataParallel`, you might encounter "module" prefixes in state keys:

```python
# Save model with DataParallel
torch.save(model.state_dict(), 'model.pth')

# Loading model - Option 1: If loading to a DataParallel model
new_model = nn.DataParallel(SimpleModel())
new_model.load_state_dict(torch.load('model.pth'))

# Loading model - Option 2: If loading to a non-DataParallel model
new_model = SimpleModel()
state_dict = torch.load('model.pth')
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] if k.startswith('module.') else k  # Remove 'module.' prefix
    new_state_dict[name] = v
new_model.load_state_dict(new_state_dict)
```

### 3. Imbalanced GPU Utilization

Sometimes, different GPUs have different utilization levels. This might happen due to:
- One GPU handling additional tasks (like driving display)
- Non-uniform data processing
- System configuration issues

Monitor your GPU usage and try setting different device orders if needed.

## Limitations of DataParallel

While `DataParallel` is easy to implement, it has some limitations:

1. **Performance Overhead**: All operations are synchronized through the main GPU, which can create a bottleneck.

2. **Memory Constraints**: The model needs to fit on a single GPU.

3. **Limited to Single Machine**: Cannot scale beyond the GPUs in a single machine.

4. **GIL Issues**: Python's Global Interpreter Lock can limit performance gains.

For more complex distributed training needs, consider using `DistributedDataParallel` (DDP), which we'll cover in the next tutorial.

## When to Use Alternatives

Consider alternatives to `DataParallel` when:
- You need to train across multiple machines
- You're experiencing significant overhead with `DataParallel`
- Your model is too large to fit on a single GPU
- You need more precise control over the distribution strategy

## Summary

`DataParallel` is an easy way to use multiple GPUs for training your PyTorch models:

- It automatically splits your data across available GPUs
- Implementation requires minimal code changes
- It's ideal for single-machine multi-GPU setups
- It helps reduce training time and allows larger batch sizes
- It has some limitations that might make alternatives like `DistributedDataParallel` more suitable for advanced use cases

By understanding how `DataParallel` works, you can effectively leverage multiple GPUs to speed up your deep learning training pipeline.

## Exercises

1. Benchmark a model training with and without `DataParallel` on your system. Compare training times and memory usage.

2. Experiment with different batch sizes when using `DataParallel`. How does batch size affect training speed and accuracy?

3. Modify the ResNet example to save the trained model and then load it back correctly without `DataParallel`.

4. Create a training loop that prints the individual GPU utilization statistics at regular intervals.

5. Compare the performance of `DataParallel` vs. manually splitting your data and managing multiple models.

## Additional Resources

- [PyTorch DataParallel Documentation](https://pytorch.org/docs/stable/nn.html#dataparallel)
- [NVIDIA Multi-GPU Training Guide](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
- [PyTorch Distributed Training Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Efficient Multi-GPU training with PyTorch](https://pytorch.org/tutorials/beginner/dist_overview.html)

Happy training with multiple GPUs!