---
title: TensorFlow API Hierarchy
description: Understanding the organization and structure of TensorFlow's API layers to effectively build machine learning models

---

# TensorFlow API Hierarchy

## Introduction

TensorFlow, one of the most popular machine learning frameworks, offers a multi-layered API structure that caters to different use cases and expertise levels. Understanding this hierarchy is crucial for effectively using TensorFlow in your machine learning projects. Think of TensorFlow's API hierarchy like a pyramid - with high-level, user-friendly APIs at the top, and lower-level, more flexible APIs at the bottom. This guide will walk through the different layers of TensorFlow's API, explaining when and how to use each one.

## The TensorFlow API Layers

TensorFlow's API is organized into several layers, each offering different levels of abstraction and control:

```
High-level API (Most abstraction)
↓
Mid-level API
↓
Low-level API (Most control)
```

Let's explore each level in detail.

### High-Level APIs: Keras

At the top of the hierarchy sits Keras, TensorFlow's high-level API designed for quickly building and training neural networks.

#### Keras API Overview

Keras is user-friendly, modular, and extensible, making it perfect for beginners and rapid prototyping.

```python
# Simple neural network using Keras Sequential API
import tensorflow as tf

# Create a sequential model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Print model summary
model.summary()
```

**Output:**
```
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 128)               100480    
                                                                 
 dropout (Dropout)           (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
_________________________________________________________________
```

#### Keras Functional API

For more complex models with non-sequential data flow, Keras offers the Functional API:

```python
# Using the Functional API for a multi-input model
import tensorflow as tf

# Define inputs
input_1 = tf.keras.Input(shape=(784,))
input_2 = tf.keras.Input(shape=(10,))

# Process inputs
x1 = tf.keras.layers.Dense(128, activation='relu')(input_1)
x2 = tf.keras.layers.Dense(128, activation='relu')(input_2)

# Combine inputs
combined = tf.keras.layers.concatenate([x1, x2])

# Output layer
output = tf.keras.layers.Dense(1, activation='sigmoid')(combined)

# Create model
model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)

# Print model info
print(f"Model inputs: {model.inputs}")
print(f"Model outputs: {model.outputs}")
```

**Output:**
```
Model inputs: [<KerasTensor shape=(None, 784) dtype=float32>, <KerasTensor shape=(None, 10) dtype=float32>]
Model outputs: [<KerasTensor shape=(None, 1) dtype=float32>]
```

### Mid-Level APIs

#### Feature Columns

Feature columns bridge raw data and your model, serving as a specification for how the model should interpret input data:

```python
# Using feature columns for structured data
import tensorflow as tf

# Numeric feature column
age = tf.feature_column.numeric_column("age")

# Bucketized feature column
age_buckets = tf.feature_column.bucketized_column(
    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]
)

# Categorical feature column with vocabulary list
education = tf.feature_column.categorical_column_with_vocabulary_list(
    'education', ['Bachelors', 'Masters', 'PhD', 'High School']
)

# Convert to indicator column (one-hot encoding)
education_one_hot = tf.feature_column.indicator_column(education)

# Show feature column information
print(f"Age buckets: {age_buckets}")
print(f"Education one-hot: {education_one_hot}")
```

**Output:**
```
Age buckets: BucketizedColumn(source_column=NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))
Education one-hot: IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'Masters', 'PhD', 'High School'), dtype=tf.string, default_value=-1, num_oov_buckets=0))
```

#### Datasets

TensorFlow's `tf.data` API helps create efficient data pipelines for feeding data to your models:

```python
# Creating an efficient data pipeline with tf.data
import tensorflow as tf
import numpy as np

# Create sample data
data = np.random.sample((1000, 3))
labels = np.random.sample((1000, 1))

# Create a dataset
dataset = tf.data.Dataset.from_tensor_slices((data, labels))

# Shuffle and batch
dataset = dataset.shuffle(buffer_size=1000).batch(32)

# Prefetch data for performance
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# Examine one batch
for features, label in dataset.take(1):
    print(f"Feature batch shape: {features.shape}")
    print(f"Label batch shape: {label.shape}")
    print(f"Sample features: {features[0]}")
```

**Output:**
```
Feature batch shape: (32, 3)
Label batch shape: (32, 1)
Sample features: [0.123 0.456 0.789]  # Actual values will vary
```

### Low-Level APIs

#### Modules, Layers, and Models

These are the building blocks used to construct custom components:

```python
# Creating a custom layer
import tensorflow as tf

class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
    
    def build(self, input_shape):
        self.kernel = self.add_weight(
            "kernel", 
            shape=[int(input_shape[-1]), self.num_outputs]
        )
    
    def call(self, inputs):
        return tf.matmul(inputs, self.kernel)

# Use the custom layer
layer = MyDenseLayer(10)
input_tensor = tf.constant([[1.0, 2.0, 3.0, 4.0]])
output = layer(input_tensor)

print(f"Input shape: {input_tensor.shape}")
print(f"Output shape: {output.shape}")
```

**Output:**
```
Input shape: (1, 4)
Output shape: (1, 10)
```

#### Automatic Differentiation with GradientTape

TensorFlow's automatic differentiation capabilities through `GradientTape` allow for custom training loops:

```python
# Custom training loop with GradientTape
import tensorflow as tf

# Create a simple model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(8, input_shape=(4,), activation='relu'),
    tf.keras.layers.Dense(1)
])

# Create an optimizer
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

# Sample data
inputs = tf.random.normal((32, 4))
targets = tf.random.normal((32, 1))

# Custom training step
def train_step(inputs, targets):
    with tf.GradientTape() as tape:
        # Forward pass
        predictions = model(inputs)
        # Compute loss
        loss = tf.reduce_mean(tf.square(predictions - targets))
    
    # Compute gradients
    gradients = tape.gradient(loss, model.trainable_variables)
    
    # Apply gradients
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    return loss

# Run one training step
loss = train_step(inputs, targets)
print(f"Loss after one step: {loss.numpy()}")
```

**Output:**
```
Loss after one step: 1.2345  # Actual value will vary
```

### Lowest-Level API: TensorFlow Core

At the very foundation of TensorFlow is its core API which includes Tensors, Variables, and mathematical operations.

```python
# Working with TensorFlow Core: Tensors and Operations
import tensorflow as tf

# Create tensors
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Basic operations
add = tf.add(a, b)
multiply = tf.matmul(a, b)
transpose = tf.transpose(b)

print(f"Tensor a:\n {a.numpy()}")
print(f"Tensor b:\n {b.numpy()}")
print(f"a + b:\n {add.numpy()}")
print(f"a × b:\n {multiply.numpy()}")
print(f"b transposed:\n {transpose.numpy()}")
```

**Output:**
```
Tensor a:
 [[1 2]
 [3 4]]
Tensor b:
 [[5 6]
 [7 8]]
a + b:
 [[ 6  8]
 [10 12]]
a × b:
 [[19 22]
 [43 50]]
b transposed:
 [[5 7]
 [6 8]]
```

## Real-World Application: Image Classification

Let's see how the API hierarchy is used in a practical example of image classification:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Load and prepare MNIST dataset
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values
x_train, x_test = x_train / 255.0, x_test / 255.0

# Reshape data for model
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Convert targets to one-hot
y_train = tf.one_hot(y_train, depth=10)
y_test = tf.one_hot(y_test, depth=10)

# Create a data pipeline
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)

# Build a CNN model using Keras
model = tf.keras.Sequential([
    # High-level Keras API for model structure
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Mid-level API: compiled with loss function, optimizer and metrics
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Print model summary
model.summary()

# Train the model (only 2 epochs for demonstration)
print("Training the model...")
history = model.fit(train_dataset, epochs=2, validation_data=test_dataset)

# Evaluate the model
print("\nEvaluating the model...")
model.evaluate(test_dataset)

# Low-level API: Make a prediction with core TensorFlow operations
print("\nMaking predictions with TensorFlow core operations...")
for images, _ in test_dataset.take(1):
    # Get first image
    image = images[0:1]
    
    # Make prediction using the model
    logits = model(image, training=False)
    
    # Convert logits to prediction using TF core operations
    prediction = tf.argmax(logits, axis=1).numpy()[0]
    
    # Display image and prediction
    plt.figure(figsize=(3, 3))
    plt.imshow(image[0, :, :, 0], cmap='gray')
    plt.title(f"Model Prediction: {prediction}")
    print(f"Predicted digit: {prediction}")
```

## When to Use Each API Level

Understanding when to use each layer of the TensorFlow API hierarchy is crucial:

1. **High-level Keras API**
   - ✅ Use for standard model architectures
   - ✅ Perfect for beginners and rapid prototyping
   - ✅ Good for most production use cases

2. **Mid-level APIs**
   - ✅ Use when you need custom data pipelines
   - ✅ When working with structured or heterogeneous data
   - ✅ For performance optimization of input pipelines

3. **Low-level APIs**
   - ✅ When implementing custom layers or training procedures
   - ✅ For research purposes requiring fine-grained control
   - ✅ When working with complex model architectures

4. **TensorFlow Core**
   - ✅ For implementing advanced algorithms from scratch
   - ✅ When you need maximum control over memory and computation
   - ✅ For specialized mathematical operations

## Summary

TensorFlow's API hierarchy provides various levels of abstraction to match your needs:

- **Keras**: High-level, user-friendly API for rapid development
- **Mid-level APIs**: Tools for data processing and feature engineering
- **Low-level APIs**: Custom layers, models, and training loops
- **TensorFlow Core**: Direct manipulation of tensors and operations

As you grow in your machine learning journey, you'll find yourself moving between these layers as needed. Beginners can start with Keras for simplicity, while gradually exploring lower levels as they require more customization and control.

## Additional Resources

- [Official TensorFlow Guide](https://www.tensorflow.org/guide)
- [Keras Documentation](https://keras.io/api/)
- [TensorFlow Datasets](https://www.tensorflow.org/datasets)
- [TensorFlow GitHub Repository](https://github.com/tensorflow/tensorflow)

## Practice Exercises

1. Create a simple image classifier using Keras Sequential API
2. Build a custom data pipeline using `tf.data` for your own dataset
3. Implement a custom training loop using `GradientTape`
4. Create a custom layer that performs a unique operation not available in standard layers
5. Experiment with different levels of the API on the same problem and compare the code complexity and performance