---
title: TensorFlow TensorBoard
description: Learn how to use TensorBoard to visualize and debug your TensorFlow models, track metrics, and optimize performance.

---

# TensorFlow TensorBoard

TensorBoard is TensorFlow's visualization toolkit that helps you understand, debug, and optimize your machine learning models. Think of it as a dashboard for your neural networks that allows you to track experiments, visualize metrics, and gain insights into your model's behavior.

## What is TensorBoard?

TensorBoard provides a suite of visualization tools to make understanding deep learning models easier:

- **Tracking metrics**: Monitor training and validation metrics like loss and accuracy in real-time
- **Visualizing model graphs**: See your model architecture as a computational graph
- **Viewing histograms**: Analyze weight distributions and how they change over time
- **Projecting embeddings**: Visualize high-dimensional data in lower dimensions
- **Profiling performance**: Identify bottlenecks in your training process

## Setting Up TensorBoard

Let's start by installing TensorBoard if you haven't already:

```bash
pip install tensorboard
```

TensorBoard is included with TensorFlow installations, but it's good practice to ensure you have the latest version.

## Basic TensorBoard Usage

### 1. Creating TensorBoard Callback

The easiest way to use TensorBoard with Keras models is through the `TensorBoard` callback:

```python
import tensorflow as tf
import datetime

# Create a log directory with timestamp
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

# Create a TensorBoard callback
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,  # How often to log histogram visualizations
    write_graph=True,  # Whether to visualize the graph
    update_freq='epoch'  # 'batch' or 'epoch' or integer
)

# Create and train a model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Include the callback during training
model.fit(
    x_train, y_train,
    epochs=10,
    validation_data=(x_val, y_val),
    callbacks=[tensorboard_callback]
)
```

### 2. Launching TensorBoard

Once your model starts training, you can launch TensorBoard to monitor progress:

```bash
tensorboard --logdir=logs/fit
```

TensorBoard will start a web server (typically on port 6006) which you can access in your browser at `http://localhost:6006`.

## Advanced TensorBoard Features

### Using tf.summary for Custom Logging

For more control over what gets logged, you can use the `tf.summary` API:

```python
import tensorflow as tf
import numpy as np

# Create a writer for logs
writer = tf.summary.create_file_writer("logs/custom_logs")

# Generate some random training data
for step in range(100):
    # Generate synthetic data (simulating training)
    x = np.random.normal(size=(100, 10))
    y_true = (np.sum(x, axis=1) > 0).astype(np.int32)
    loss = np.random.random() * 0.5 + 0.1 * step/100
    accuracy = 0.5 + step/200  # Simulated accuracy improving over time
    
    # Log metrics manually
    with writer.as_default():
        tf.summary.scalar("loss", loss, step=step)
        tf.summary.scalar("accuracy", accuracy, step=step)
        
        # Log a histogram of a particular layer's weights
        weights = np.random.normal(size=(10, 10))
        tf.summary.histogram("weights", weights, step=step)
```

### Visualizing the Model Graph

TensorBoard can display your model architecture:

```python
import tensorflow as tf

# Create a model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Create a log writer
log_dir = "logs/model_graph"
writer = tf.summary.create_file_writer(log_dir)

# Log the graph
tf.summary.trace_on(graph=True)
# Call the model once to generate the graph
model(tf.zeros((1, 28, 28, 1)))
with writer.as_default():
    tf.summary.trace_export(name="model_trace", step=0)
```

### Image Visualization

TensorBoard can also display images, which is useful for tasks like computer vision:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Create some test images (3 random images)
test_images = np.random.random((3, 28, 28)) * 255
test_images = test_images.astype(np.uint8)

# Create a file writer
log_dir = "logs/image_examples"
writer = tf.summary.create_file_writer(log_dir)

# Log the images
with writer.as_default():
    # Log the images, converting to uint8 if needed
    tf.summary.image("Test Images", test_images[:, :, :, np.newaxis], step=0, max_outputs=3)
```

## Real-World Example: Monitoring a Convolutional Neural Network

Let's put everything together with a practical example using the MNIST dataset:

```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import datetime

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Add a channel dimension
train_images = train_images[..., tf.newaxis]
test_images = test_images[..., tf.newaxis]

# Define the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Set up TensorBoard logging
log_dir = "logs/mnist_cnn/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    write_graph=True,
    write_images=True,
    update_freq='epoch',
    profile_batch=2
)

# Train the model with TensorBoard monitoring
history = model.fit(
    train_images, train_labels,
    epochs=5,
    validation_data=(test_images, test_labels),
    callbacks=[tensorboard_callback]
)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f"Test accuracy: {test_acc}")
```

## Using TensorBoard with Multiple Experiments

One of TensorBoard's strengths is comparing different model configurations:

```python
import tensorflow as tf
import datetime

# Function to create and train a model with different parameters
def create_and_train_model(hidden_layers, learning_rate, name):
    # Create log directory
    log_dir = f"logs/comparison/{name}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    # Build the model based on parameters
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
    
    # Add hidden layers with specified neurons
    for neurons in hidden_layers:
        model.add(tf.keras.layers.Dense(neurons, activation='relu'))
    
    # Output layer
    model.add(tf.keras.layers.Dense(10, activation='softmax'))
    
    # Compile with the specified learning rate
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Create callback
    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=log_dir,
        histogram_freq=1
    )
    
    # Train the model
    model.fit(
        x_train, y_train,
        epochs=5,
        validation_data=(x_test, y_test),
        callbacks=[tensorboard_callback]
    )

# Load MNIST data (simplified code)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Train different model configurations
create_and_train_model([128], 0.001, "small_lr_0.001")
create_and_train_model([128], 0.01, "small_lr_0.01") 
create_and_train_model([256, 128], 0.001, "large_lr_0.001")
create_and_train_model([256, 128], 0.01, "large_lr_0.01")
```

To view and compare these experiments in TensorBoard:

```bash
tensorboard --logdir=logs/comparison
```

## Profiling Performance with TensorBoard

TensorBoard includes tools to identify performance bottlenecks:

```python
import tensorflow as tf

# Load and prepare the dataset
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / 255.0

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Set up TensorBoard with profiling enabled
log_dir = "logs/profile"

# Create a TensorBoard callback with profiling
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    profile_batch='500,520'  # Profile from batch 500 to 520
)

# Train the model with profiling
model.fit(
    x_train, y_train,
    epochs=2,
    batch_size=64,
    callbacks=[tensorboard_callback]
)
```

## Summary

TensorBoard is an essential tool for any TensorFlow developer, providing rich visualizations that help you:

- Monitor training progress and model performance
- Debug issues in your models
- Compare different model architectures and hyperparameters
- Optimize training performance
- Visualize complex data and model structures

By integrating TensorBoard into your machine learning workflow, you gain valuable insights that can help you develop better models more efficiently.

## Additional Resources

- [Official TensorBoard documentation](https://www.tensorflow.org/tensorboard)
- [TensorBoard GitHub repository](https://github.com/tensorflow/tensorboard)
- [TensorFlow tutorials on TensorBoard](https://www.tensorflow.org/tensorboard/get_started)

## Exercises

1. Create a simple neural network for the MNIST dataset and use TensorBoard to visualize the training metrics.
2. Compare two different model architectures in TensorBoard side by side.
3. Use TensorBoard's profiler to identify performance bottlenecks in your model.
4. Visualize image predictions over time during training using TensorBoard's image logging features.
5. Create a custom visualization in TensorBoard using the `tf.summary` API to track a specific aspect of your model's training.