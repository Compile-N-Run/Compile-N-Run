---
title: TensorFlow Placeholders
description: Learn about placeholders in TensorFlow and how they allow you to feed data into your computational graph during execution

---

# TensorFlow Placeholders

## Introduction

In TensorFlow 1.x, placeholders are special nodes in your computational graph that allow you to feed external data into the graph during runtime. Think of placeholders as "empty variables" that you define in your graph, promising to provide values for them when you execute the graph. This is particularly useful when working with training data that you want to pass in batches.

> **Note:** With TensorFlow 2.x's eager execution, placeholders are less common, but understanding them is still valuable, especially if you're working with legacy code or using TensorFlow 1.x. This tutorial will primarily focus on the TensorFlow 1.x approach.

## Basic Usage of Placeholders

### Creating a Placeholder

You can create a placeholder using `tf.placeholder()` function, specifying its data type and optionally its shape:

```python
import tensorflow as tf

# Disable eager execution to use placeholders
tf.compat.v1.disable_eager_execution()

# Create a placeholder for floating point numbers
x = tf.compat.v1.placeholder(tf.float32, shape=(None, 3), name="input_data")
```

In this example:
- We've created a placeholder `x` for floating-point values
- The `shape=(None, 3)` means it will accept batches of any size, where each item has 3 features
- The `None` dimension allows flexibility in the batch size
- We've given it a name "input_data" for easier identification in the computational graph

### Feeding Values to Placeholders

Placeholders must be fed with values when running operations that depend on them:

```python
# Create a simple operation using our placeholder
y = tf.square(x)

# Create a session
with tf.compat.v1.Session() as sess:
    # Create sample data: 2 examples, each with 3 features
    data = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
    
    # Run the operation while feeding the placeholder
    result = sess.run(y, feed_dict={x: data})
    
    print("Input data:")
    print(data)
    print("\nSquared values:")
    print(result)
```

Output:
```
Input data:
[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]

Squared values:
[[ 1.  4.  9.]
 [16. 25. 36.]]
```

The `feed_dict` parameter is a dictionary that maps placeholder operations to the values we want to feed them.

## Multiple Placeholders

You can use multiple placeholders in your graph for different inputs:

```python
# Create placeholders
a = tf.compat.v1.placeholder(tf.float32, name="input_a")
b = tf.compat.v1.placeholder(tf.float32, name="input_b")

# Create operations
add_op = a + b
mul_op = a * b

# Create a session
with tf.compat.v1.Session() as sess:
    # Feed values to both placeholders
    add_result, mul_result = sess.run(
        [add_op, mul_op], 
        feed_dict={a: 3.0, b: 4.0}
    )
    
    print(f"Addition: {add_result}")
    print(f"Multiplication: {mul_result}")
```

Output:
```
Addition: 7.0
Multiplication: 12.0
```

## Placeholders with Different Data Types

Placeholders can handle various data types:

```python
# Integer placeholder
int_placeholder = tf.compat.v1.placeholder(tf.int32, name="integers")

# String placeholder
str_placeholder = tf.compat.v1.placeholder(tf.string, name="strings")

# Boolean placeholder
bool_placeholder = tf.compat.v1.placeholder(tf.bool, name="booleans")

# Create operations
int_doubled = int_placeholder * 2
str_concat = tf.strings.join([str_placeholder, " World"])
not_bool = tf.logical_not(bool_placeholder)

with tf.compat.v1.Session() as sess:
    # Run all operations with different data types
    results = sess.run(
        [int_doubled, str_concat, not_bool],
        feed_dict={
            int_placeholder: 42,
            str_placeholder: "Hello",
            bool_placeholder: True
        }
    )
    
    int_result, str_result, bool_result = results
    print(f"Integer doubled: {int_result}")
    print(f"String concatenated: {str_result}")
    print(f"Boolean negated: {bool_result}")
```

Output:
```
Integer doubled: 84
String concatenated: Hello World
Boolean negated: False
```

## Practical Example: Linear Regression with Placeholders

Let's implement a simple linear regression model using placeholders:

```python
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.7 + 0.3 + np.random.normal(0, 0.05, 100).astype(np.float32)

# Create placeholders for input and output data
x = tf.compat.v1.placeholder(tf.float32, name="x")
y_true = tf.compat.v1.placeholder(tf.float32, name="y_true")

# Define model parameters (variables)
W = tf.Variable(tf.random.normal([1]), name="weight")
b = tf.Variable(tf.zeros([1]), name="bias")

# Define the linear model
y_pred = W * x + b

# Define loss function (mean squared error)
loss = tf.reduce_mean(tf.square(y_pred - y_true))

# Define optimizer
learning_rate = 0.1
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
train_op = optimizer.minimize(loss)

# Create a session and initialize variables
with tf.compat.v1.Session() as sess:
    # Initialize variables
    sess.run(tf.compat.v1.global_variables_initializer())
    
    # Training loop
    for step in range(200):
        # Feed the entire dataset in each step for simplicity
        _, loss_val, W_val, b_val = sess.run(
            [train_op, loss, W, b],
            feed_dict={x: x_data, y_true: y_data}
        )
        
        if step % 20 == 0:
            print(f"Step: {step}, Loss: {loss_val:.4f}, W: {W_val[0]:.4f}, b: {b_val[0]:.4f}")
    
    # Get final parameters
    final_W, final_b = sess.run([W, b])
    
    # Plot the results
    plt.scatter(x_data, y_data, label='Data')
    plt.plot(x_data, final_W * x_data + final_b, 'r-', linewidth=2, label='Fitted Line')
    plt.title('Linear Regression with TensorFlow')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.show()
    
    print(f"True values - Weight: 0.7, Bias: 0.3")
    print(f"Learned values - Weight: {final_W[0]:.4f}, Bias: {final_b[0]:.4f}")
```

Sample output:
```
Step: 0, Loss: 0.2200, W: 0.6412, b: 0.0839
Step: 20, Loss: 0.0031, W: 0.6943, b: 0.3037
Step: 40, Loss: 0.0030, W: 0.6921, b: 0.3073
...
Step: 180, Loss: 0.0029, W: 0.6895, b: 0.3113
True values - Weight: 0.7, Bias: 0.3
Learned values - Weight: 0.6895, Bias: 0.3114
```

## When to Use Placeholders

Placeholders are particularly useful when:

1. **Feeding training data**: Batch-wise training requires feeding different data in each training step
2. **Building reusable models**: You can create a model architecture once and feed different data through it
3. **Performing inference**: After training, you can use placeholders to feed new data for predictions

## Placeholders vs. Variables

It's important to understand the difference between placeholders and variables:

| **Placeholder** | **Variable** |
| --- | --- |
| No value until fed with `feed_dict` | Initialized with a value |
| Cannot be trained or updated | Can be updated through training |
| Used for external input data | Used for learnable parameters |
| Don't need initialization | Need to be explicitly initialized |

## TensorFlow 2.x Approach

In TensorFlow 2.x, placeholders have been replaced with a more straightforward approach using functions and eager execution:

```python
import tensorflow as tf

# Define a model function that takes inputs
def simple_model(x):
    # Model weights
    W = tf.Variable([0.3])
    b = tf.Variable([0.3])
    return W * x + b

# Use the function with different inputs
x_value = tf.constant([1.0, 2.0, 3.0])
result = simple_model(x_value)
print(result.numpy())
```

This approach is more intuitive and Python-like, allowing for better debugging and more natural code structure.

## Summary

Placeholders are a fundamental concept in TensorFlow 1.x that allow you to:

- Define the structure of your input data in advance
- Feed different data into your computational graph at runtime
- Create flexible, reusable models
- Process data in batches during training

While TensorFlow 2.x has moved away from placeholders with its eager execution paradigm, understanding this concept is still valuable, especially when working with legacy code or when you need to use graph mode for performance optimization.

## Additional Resources

- [TensorFlow Documentation on Placeholders](https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder)
- [Migrate TensorFlow 1.x to 2.x](https://www.tensorflow.org/guide/migrate)
- [TensorFlow Graph Execution](https://www.tensorflow.org/guide/intro_to_graphs)

## Exercises

1. Create a placeholder for images with shape `[None, 28, 28, 1]` (MNIST-like grayscale images) and feed it with random data
2. Implement a simple neural network model using placeholders for both features and labels
3. Modify the linear regression example to use mini-batches of size 10 instead of the entire dataset
4. Create a placeholder with a specific shape and intentionally feed data with an incorrect shape. Observe the error and understand why it occurs