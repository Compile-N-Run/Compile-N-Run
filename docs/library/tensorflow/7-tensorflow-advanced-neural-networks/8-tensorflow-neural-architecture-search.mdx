---
title: TensorFlow Neural Architecture Search
description: Learn how to use Neural Architecture Search (NAS) in TensorFlow to automatically discover optimal neural network architectures for your machine learning tasks.

---

# TensorFlow Neural Architecture Search

## Introduction

Neural Architecture Search (NAS) is an exciting field of machine learning that focuses on automating the design of neural network architectures. Instead of manually designing neural networks through trial and error, NAS uses algorithms to discover optimal architectures for specific tasks automatically. This approach is part of the broader AutoML (Automated Machine Learning) paradigm that aims to make AI more accessible by automating the model creation process.

In this tutorial, we'll explore how to implement Neural Architecture Search using TensorFlow, Google's popular deep learning framework. By the end of this lesson, you'll understand the key concepts behind NAS and be able to use TensorFlow's NAS capabilities to build more efficient neural networks.

## Understanding Neural Architecture Search

### What is Neural Architecture Search?

Neural Architecture Search is the process of automatically discovering the best neural network architecture for a specific task. Traditional deep learning requires extensive manual experimentation to find optimal architectures, which can be time-consuming and requires significant expertise. NAS addresses this challenge by:

1. **Automating architecture design**: Algorithms explore the space of possible architectures
2. **Optimizing for performance**: Architectures are evaluated based on accuracy, efficiency, and other metrics
3. **Reducing human bias**: Machine-driven search may discover novel architectures humans might overlook

### Key Components of NAS

A NAS system typically consists of three main components:

1. **Search space**: Defines the possible architectures that can be explored
2. **Search strategy**: Algorithm that explores the search space (e.g., reinforcement learning, evolution, gradient-based methods)
3. **Performance estimation strategy**: Method to evaluate candidate architectures

## TensorFlow Neural Architecture Search Libraries

TensorFlow offers several libraries and tools for NAS:

### 1. TensorFlow Model Optimization Toolkit

The TensorFlow Model Optimization Toolkit includes utilities for pruning, quantization, and architecture search.

### 2. Keras Tuner

Keras Tuner provides hyperparameter tuning capabilities, which can be extended to search for architectural parameters.

### 3. TF-Agents

For reinforcement learning-based NAS approaches, TF-Agents provides a framework for training agents to discover optimal architectures.

Let's explore a simple example using Keras Tuner to perform a limited form of Neural Architecture Search.

## Basic NAS with Keras Tuner

Keras Tuner allows us to define a search space for our neural network architecture and then systematically explore that space to find optimal configurations.

### Step 1: Installation

First, let's install Keras Tuner:

```bash
pip install keras-tuner
```

### Step 2: Define a Model-Building Function with a Search Space

```python
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt

def build_model(hp):
    """Define a model with a search space"""
    model = keras.Sequential()
    
    # Input layer
    model.add(keras.layers.Flatten(input_shape=(28, 28)))
    
    # Tune the number of layers and units per layer
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(keras.layers.Dense(
            units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),
            activation='relu'
        ))
        
        # Optional dropout
        if hp.Boolean(f'dropout_{i}'):
            model.add(keras.layers.Dropout(
                rate=hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1)
            ))
    
    # Output layer
    model.add(keras.layers.Dense(10, activation='softmax'))
    
    # Compile the model
    model.compile(
        optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model
```

### Step 3: Set Up the Tuner

```python
# Initialize the tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=2,
    directory='my_dir',
    project_name='nas_tutorial'
)
```

### Step 4: Perform the Search

```python
# Load dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize pixel values
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Search for the best model
tuner.search(
    x_train, y_train,
    epochs=5,
    validation_split=0.2,
    callbacks=[keras.callbacks.EarlyStopping(patience=1)]
)

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluate the best model
test_loss, test_acc = best_model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_acc:.3f}')

# Print the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best hyperparameters:")
for hp in best_hps.values:
    print(f"- {hp}: {best_hps.get(hp)}")
```

### Expected Output:

```
Trial 10 Complete [00h 01m 47s]
val_accuracy: 0.9760833382606506

Best val_accuracy So Far: 0.9783333539962769
Total elapsed time: 00h 16m 34s

313/313 [==============================] - 0s 931us/step - loss: 0.0752 - accuracy: 0.9776
Test accuracy: 0.978
Best hyperparameters:
- num_layers: 2
- units_0: 128
- dropout_0: True
- dropout_rate_0: 0.2
- units_1: 64
- dropout_1: False
- optimizer: adam
```

## Advanced NAS Techniques in TensorFlow

While Keras Tuner provides a simple way to implement basic architectural search, more sophisticated NAS approaches are available for advanced users.

### Weight-Sharing NAS with TF-NAS

TensorFlow's Neural Architecture Search (TF-NAS) implements efficient search techniques like weight-sharing NAS. Let's examine a conceptual implementation:

```python
# This is a conceptual example of weight-sharing NAS
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

# Define a supernet with multiple architectural options
class SearchableBlock(layers.Layer):
    def __init__(self, filters_options, kernel_options):
        super(SearchableBlock, self).__init__()
        self.paths = []
        
        # Create different convolutional paths
        for filters in filters_options:
            for kernel_size in kernel_options:
                self.paths.append(
                    layers.Conv2D(filters, kernel_size, padding='same', activation='relu')
                )
        
        # Architecture parameters (to be learned)
        self.path_logits = tf.Variable(
            initial_value=tf.zeros(len(self.paths)),
            trainable=True,
            name='path_logits'
        )
    
    def call(self, inputs, training=None):
        # Apply softmax to get path weights
        path_weights = tf.nn.softmax(self.path_logits)
        
        # Compute weighted sum of all paths
        outputs = 0
        for i, path in enumerate(self.paths):
            outputs += path_weights[i] * path(inputs)
            
        return outputs
```

### Progressive Neural Architecture Search (PNAS)

PNAS is a more efficient approach compared to standard NAS. It starts with simple structures and incrementally builds complexity:

```python
# Conceptual PNAS implementation
def progressive_search():
    # Start with a set of simple cells
    cells = get_initial_simple_cells()
    
    # Initialize performance predictor
    predictor = train_performance_predictor(cells)
    
    # Progressively search for more complex cells
    for complexity_level in range(max_complexity):
        # Generate candidates for the next level of complexity
        candidates = expand_cells(cells)
        
        # Predict performance of candidates
        predicted_performance = predictor.predict(candidates)
        
        # Select top K candidates
        top_k_candidates = select_top_k(candidates, predicted_performance)
        
        # Evaluate actual performance of top K and update predictor
        actual_performance = evaluate(top_k_candidates)
        predictor.update(top_k_candidates, actual_performance)
        
        # Update cells for next iteration
        cells = top_k_candidates
    
    return cells[0]  # Return best cell
```

## Real-World Application: NAS for Image Classification

Let's implement a more complete example of using Neural Architecture Search for image classification on the CIFAR-10 dataset:

```python
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt
import time

# Load and prepare the CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = y_train.flatten()
y_test = y_test.flatten()

# Define a CNN model with searchable hyperparameters
def build_cnn_model(hp):
    model = keras.Sequential()
    
    # Initial convolutional layer
    model.add(keras.layers.Conv2D(
        filters=hp.Int('initial_filters', 32, 128, step=32),
        kernel_size=hp.Choice('initial_kernel', values=[3, 5]),
        activation='relu',
        padding='same',
        input_shape=(32, 32, 3)
    ))
    
    # Add convolutional blocks
    for i in range(hp.Int('conv_blocks', 1, 3)):
        filters = hp.Int(f'filters_{i}', 32, 256, step=32)
        
        for j in range(hp.Int(f'layers_in_block_{i}', 1, 3)):
            model.add(keras.layers.Conv2D(
                filters=filters,
                kernel_size=hp.Choice(f'kernel_size_{i}_{j}', values=[3, 5]),
                activation='relu',
                padding='same'
            ))
        
        # Add pooling after each block
        pool_type = hp.Choice(f'pooling_{i}', ['max', 'avg'])
        if pool_type == 'max':
            model.add(keras.layers.MaxPooling2D())
        else:
            model.add(keras.layers.AveragePooling2D())
        
        # Optionally add dropout
        if hp.Boolean(f'dropout_{i}'):
            model.add(keras.layers.Dropout(
                rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)
            ))
    
    # Flattening and dense layers
    model.add(keras.layers.Flatten())
    
    # Add dense layers
    for i in range(hp.Int('dense_layers', 0, 2)):
        model.add(keras.layers.Dense(
            units=hp.Int(f'dense_units_{i}', 64, 512, step=64),
            activation='relu'
        ))
        
        if hp.Boolean(f'dense_dropout_{i}'):
            model.add(keras.layers.Dropout(
                rate=hp.Float(f'dense_dropout_rate_{i}', 0.1, 0.5, step=0.1)
            ))
    
    # Output layer
    model.add(keras.layers.Dense(10, activation='softmax'))
    
    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')
        ),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Create the tuner
tuner = kt.BayesianOptimization(
    build_cnn_model,
    objective='val_accuracy',
    max_trials=20,
    directory='nas_cifar10',
    project_name='cifar10_cnn_search'
)

# Define callbacks
callbacks = [
    keras.callbacks.EarlyStopping(patience=3),
    keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2)
]

# Start the search
start_time = time.time()
print("Starting Neural Architecture Search...")

tuner.search(
    x_train, y_train,
    validation_split=0.2,
    epochs=15,
    batch_size=64,
    callbacks=callbacks
)

search_time = time.time() - start_time
print(f"Neural Architecture Search completed in {search_time:.2f} seconds")

# Get the best model and hyperparameters
best_model = tuner.get_best_models(1)[0]
best_hp = tuner.get_best_hyperparameters(1)[0]

# Display the best hyperparameters
print("\nBest hyperparameters found:")
for param in best_hp.values:
    print(f"- {param}: {best_hp.get(param)}")

# Evaluate the best model
print("\nEvaluating best model on test data...")
test_loss, test_acc = best_model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

# Save the best model
best_model.save('best_nas_cifar10_model')
print("Best model saved to 'best_nas_cifar10_model'")
```

In this real-world example, we use Bayesian Optimization to efficiently search through the space of possible CNN architectures for CIFAR-10 image classification. The search space includes varying:

1. Number of convolutional blocks
2. Filters per block
3. Kernel sizes
4. Pooling types
5. Dropout configurations
6. Number of dense layers
7. Learning rate

This approach can significantly outperform manually designed architectures with much less human effort.

## Future of NAS in TensorFlow

The field of Neural Architecture Search is rapidly evolving. Here are some advanced approaches being developed in the TensorFlow ecosystem:

### 1. TensorFlow Lattice

TensorFlow Lattice incorporates NAS principles to build models that are both accurate and interpretable.

### 2. Cloud AutoML

Google Cloud offers AutoML solutions powered by TensorFlow that incorporate NAS to automatically build custom models.

### 3. Hardware-aware NAS

These techniques optimize architectures not just for accuracy but also for specific hardware constraints, such as mobile devices or edge computing platforms.

## Summary

Neural Architecture Search represents a significant advancement in the automation of deep learning model design. In this tutorial, we've covered:

1. **The fundamentals of NAS**: Search spaces, search strategies, and performance estimation
2. **Basic NAS implementation** with Keras Tuner
3. **Advanced NAS concepts** like weight-sharing and progressive architecture search
4. **A real-world application** of NAS for image classification
5. **Future directions** in the field of automated neural network design

By leveraging these techniques, you can create more efficient and effective neural networks while reducing the time spent on manual architecture design and hyperparameter tuning.

## Additional Resources

- [TensorFlow Neural Architecture Search Documentation](https://www.tensorflow.org/neural_structured_learning/tutorials/neural_architecture_search_tutorial)
- [Keras Tuner Documentation](https://keras.io/keras_tuner/)
- [Google Cloud AutoML](https://cloud.google.com/automl)
- [Neural Architecture Search: A Survey](https://arxiv.org/abs/1808.05377)
- [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)

## Exercises

1. **Basic NAS Exercise**: Modify the first Keras Tuner example to search for optimal CNN architectures for MNIST digit classification.

2. **Intermediate Exercise**: Implement a custom NAS approach using weight-sharing for a text classification task.

3. **Advanced Exercise**: Create a hardware-aware NAS implementation that optimizes both model accuracy and inference time on mobile devices.

4. **Research Project**: Compare the performance of models discovered through NAS with state-of-the-art manually designed architectures on a dataset of your choice.

By completing these exercises, you'll gain hands-on experience with Neural Architecture Search and develop skills that are increasingly valuable in the deep learning industry.