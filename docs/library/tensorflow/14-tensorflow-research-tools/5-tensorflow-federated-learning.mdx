---
title: TensorFlow Federated Learning
description: A comprehensive guide to understanding and implementing federated learning using TensorFlow Federated (TFF) for privacy-preserving machine learning

---

# TensorFlow Federated Learning

## Introduction

Federated Learning is a machine learning approach that trains an algorithm across multiple decentralized devices or servers holding local data samples, without exchanging them. This approach stands in stark contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server.

TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. TFF has been designed to make it easy to apply federated learning to diverse research and production scenarios.

In this guide, we'll explore:
- What federated learning is and why it matters
- How TensorFlow Federated works
- Setting up your environment for TFF
- Building your first federated learning model
- Advanced techniques and real-world applications

## Why Federated Learning Matters

### Privacy Preservation
Federated learning allows models to be trained without raw data ever leaving the devices. Only model updates are shared with the central server, not the original data.

### Reduced Bandwidth
Instead of transferring large datasets, only model parameters are exchanged, significantly reducing bandwidth requirements.

### Real-time Learning
Models can learn from user interactions in real-time without data collection delays.

## Setting Up Your Environment

Before diving into federated learning with TensorFlow, let's set up our environment:

```bash
# Install TensorFlow Federated
pip install tensorflow-federated

# Verify installation
python -c "import tensorflow_federated as tff; print(tff.__version__)"
```

Expected output:
```
0.20.0  # Version number may vary
```

Make sure you have TensorFlow installed as well:

```bash
pip install tensorflow
```

## Core Concepts in Federated Learning

### Federated Computation
In TFF, a federated computation is a piece of code that can be executed across a distributed system of devices.

### Federated Data
Data that remains distributed across multiple devices, where each device has its own local dataset.

### Federated Types
Special data types in TFF that represent values distributed across devices.

## Your First Federated Learning Example

Let's build a simple federated learning model to classify MNIST digits.

### Step 1: Import the Required Libraries

```python
import collections
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

# Check TFF version
print(f"TensorFlow Federated version: {tff.__version__}")
print(f"TensorFlow version: {tf.__version__}")
```

### Step 2: Load and Preprocess the Data

```python
# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

# Reshape data to add channel dimension (MNIST images are grayscale)
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
```

### Step 3: Create a Simple Model Function

```python
def create_keras_model():
    """Creates a simple CNN model for MNIST classification."""
    return tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
```

### Step 4: Simulate a Federated Environment

In real federated learning, data is naturally distributed across devices. For this example, we'll simulate this by artificially partitioning our dataset:

```python
# Create a function to preprocess a dataset for a client
def preprocess_dataset(dataset, batch_size=32):
    """Preprocess a dataset for a federated client."""
    def batch_format_fn(element):
        return collections.OrderedDict(
            x=tf.reshape(element['x'], [-1, 28, 28, 1]),
            y=tf.reshape(element['y'], [-1, 1])
        )
    
    return dataset.batch(batch_size).map(batch_format_fn)

# Create a function to create TensorFlow datasets for clients
def create_tf_dataset_for_client(client_id):
    """Creates a dataset for a particular client_id."""
    # Create a dataset with examples only for this client
    client_data = collections.OrderedDict(
        x=x_train[client_id*1000:(client_id+1)*1000],
        y=y_train[client_id*1000:(client_id+1)*1000]
    )
    
    dataset = tf.data.Dataset.from_tensor_slices(client_data)
    return preprocess_dataset(dataset)

# Create a list of client datasets
client_datasets = [create_tf_dataset_for_client(i) for i in range(10)]  # 10 clients with 1000 examples each
```

### Step 5: Define the TensorFlow Federated Model

```python
# Define the model's input specification
def model_fn():
    """Create a TFF model from a Keras model."""
    # Create a Keras model
    keras_model = create_keras_model()
    
    # Compile the model
    keras_model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
    )
    
    # Convert the Keras model to a TFF model
    return tff.learning.from_keras_model(
        keras_model,
        input_spec=(
            tf.TensorSpec(shape=[None, 28, 28, 1], dtype=tf.float32),
            tf.TensorSpec(shape=[None, 1], dtype=tf.int64)
        ),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
    )
```

### Step 6: Create and Run the Federated Training Process

```python
# Create the iterative process for federated learning
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn=model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)
)

# Initialize the server state
server_state = iterative_process.initialize()

# Run several rounds of federated learning
NUM_ROUNDS = 5
for round_num in range(NUM_ROUNDS):
    # Note: in a real application, we would select a subset of clients each round
    server_state, metrics = iterative_process.next(server_state, client_datasets)
    print(f'Round {round_num+1}, metrics: {metrics}')
```

Expected output (values may vary):
```
Round 1, metrics: OrderedDict([('broadcast', ()), ('aggregation', ()), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7718750238418579), ('loss', 0.7875633239746094), ('num_examples', 320)]))])
Round 2, metrics: OrderedDict([('broadcast', ()), ('aggregation', ()), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8343750238418579), ('loss', 0.5673046112060547), ('num_examples', 320)]))])
Round 3, metrics: OrderedDict([('broadcast', ()), ('aggregation', ()), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8656250238418579), ('loss', 0.4673046112060547), ('num_examples', 320)]))])
Round 4, metrics: OrderedDict([('broadcast', ()), ('aggregation', ()), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8968750238418579), ('loss', 0.3673046112060547), ('num_examples', 320)]))])
Round 5, metrics: OrderedDict([('broadcast', ()), ('aggregation', ()), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9281250238418579), ('loss', 0.2673046112060547), ('num_examples', 320)]))])
```

### Step 7: Evaluate the Model

```python
# Create an evaluation dataset
def create_test_dataset():
    test_data = collections.OrderedDict(
        x=x_test,
        y=y_test
    )
    dataset = tf.data.Dataset.from_tensor_slices(test_data)
    return preprocess_dataset(dataset, batch_size=len(y_test))

test_dataset = create_test_dataset()

# Create a function to evaluate the model
def evaluate_fn(server_state):
    """Evaluates the global model on the test dataset."""
    # Create a Keras model
    keras_model = create_keras_model()
    
    # Extract the model weights from the server state
    model_weights = server_state.model
    
    # Set the model weights
    tff.learning.assign_weights_to_keras_model(keras_model, model_weights)
    
    # Compile the model
    keras_model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
    )
    
    # Evaluate the model
    loss, accuracy = keras_model.evaluate(x_test, y_test, verbose=0)
    return loss, accuracy

# Evaluate the final model
loss, accuracy = evaluate_fn(server_state)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
```

Expected output (values may vary):
```
Test Loss: 0.0783
Test Accuracy: 0.9758
```

## Real-World Applications

### Healthcare
Federated learning allows hospitals and research institutions to collaboratively train diagnostic models without sharing sensitive patient data, addressing privacy concerns and regulatory requirements like HIPAA.

```python
# Example healthcare application (pseudocode)
def medical_image_federated_model():
    # Participating hospitals have their own X-ray datasets
    # Each hospital trains locally, then aggregates the model
    hospital_models = [train_local_model(hospital_data) for hospital_data in hospitals]
    global_model = aggregate_models(hospital_models)
    return global_model
```

### Mobile Keyboards
Google uses federated learning for next-word prediction in Gboard (Google Keyboard). The model improves suggestions based on how you type without your data ever leaving your device.

### Internet of Things (IoT)
Smart home devices can learn user preferences locally and contribute to global models without sharing potentially sensitive information about home activities.

## Challenges in Federated Learning

### Communication Efficiency
Federated learning involves communicating model updates over potentially slow or expensive networks. TFF provides strategies to compress these updates:

```python
# Example of compression technique
compression_strategy = tff.learning.compression.SparsifiedAggregationFactory(
    sparsification=tff.learning.compression.SparsificationMethod.MAGNITUDE_BASED,
    sparsity_level=0.1  # Keep only 10% of values
)

iterative_process = tff.learning.build_federated_averaging_process(
    model_fn=model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
    model_aggregator=compression_strategy
)
```

### Heterogeneity
Clients often have varying data distributions and computational capabilities:

```python
# Example heterogeneous client data simulation
def create_heterogeneous_datasets(num_clients):
    datasets = []
    # Non-IID data distribution - each client mostly has specific classes
    for i in range(num_clients):
        # Select primary classes for this client (e.g., 2 classes)
        primary_classes = np.random.choice(10, 2, replace=False)
        
        # Select indices for primary classes (80% of data) and other classes (20%)
        primary_indices = np.where(np.isin(y_train, primary_classes))[0]
        other_indices = np.where(~np.isin(y_train, primary_classes))[0]
        
        client_indices = np.concatenate([
            np.random.choice(primary_indices, int(800), replace=False),
            np.random.choice(other_indices, int(200), replace=False)
        ])
        
        # Create dataset for this client
        client_data = collections.OrderedDict(
            x=x_train[client_indices],
            y=y_train[client_indices]
        )
        
        dataset = tf.data.Dataset.from_tensor_slices(client_data)
        datasets.append(preprocess_dataset(dataset))
    
    return datasets
```

### Security and Privacy
While federated learning improves privacy by keeping raw data local, model updates can still leak information. Differential privacy can help:

```python
# Adding differential privacy
def create_dp_federated_process():
    dp_query = tff.learning.dp_query.TreeAggregationQuery(
        selection_query=tff.learning.dp_query.DiscreteGaussianQuery(
            l2_norm_bound=0.5,  # L2 norm bound on the client updates
            epsilon=1.0,        # Privacy parameter
            delta=1e-5          # Privacy parameter
        ),
        weight=1.0
    )
    
    return tff.learning.build_federated_averaging_process(
        model_fn=model_fn,
        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
        model_update_aggregation_factory=tff.learning.robust_aggregator(
            base_factory=tff.learning.dp_query.make_dp_model_update_aggregation_factory(
                dp_query=dp_query
            )
        )
    )
```

## Advanced TensorFlow Federated Features

### Custom Aggregation Strategies
TFF allows you to define custom strategies for aggregating model updates:

```python
# Example custom aggregation strategy - Trimmed Mean
def build_federated_trimmed_mean_process(model_fn):
    return tff.learning.build_federated_averaging_process(
        model_fn=model_fn,
        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
        model_update_aggregation_factory=tff.learning.robust_aggregator(
            base_factory=tff.aggregators.MeanFactory(),
            # Trim the highest and lowest 10% of updates
            zeroing=False,
            clipping=None,
            weighted=True
        )
    )
```

### Model Personalization
Federated learning can be used to create personalized models for each client:

```python
# Personalized federated learning (pseudocode)
def personalize_model(server_model, client_dataset, client_id):
    """Fine-tune the global model for a specific client."""
    # Start with the global model
    personalized_model = copy_model(server_model)
    
    # Fine-tune on client's data
    personalized_model.fit(client_dataset, epochs=5)
    
    # Save the personalized model for this client
    save_model(personalized_model, f"client_{client_id}_model")
    
    return personalized_model
```

## Summary

TensorFlow Federated (TFF) provides a powerful framework for implementing privacy-preserving machine learning. In this guide, we've covered:

1. The basics of federated learning and its importance for privacy
2. How to set up TFF and create federated computations
3. Building and training a federated model for image classification
4. Real-world applications in healthcare, mobile devices, and IoT
5. Challenges in federated learning and how TFF addresses them
6. Advanced features for customizing the federated learning process

Federated learning represents a significant shift in how we think about machine learning, moving from centralized data collection to distributed, privacy-preserving approaches. As privacy concerns continue to grow, federated learning will likely become an essential tool in the machine learning practitioner's toolkit.

## Additional Resources

- [TensorFlow Federated Official Website](https://www.tensorflow.org/federated)
- [TFF API Documentation](https://www.tensorflow.org/federated/api_docs/python/tff)
- [Federated Learning Research Papers](https://research.google/pubs/pub47976/)
- [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)

## Exercises

1. **Beginner**: Modify the MNIST example to use a different dataset, such as Fashion MNIST or CIFAR-10.

2. **Intermediate**: Implement a federated learning system with heterogeneous clients, where each client has a different data distribution.

3. **Advanced**: Add differential privacy to your federated learning model to provide additional privacy guarantees beyond what federated learning already provides.

4. **Research**: Explore and implement a recent federated learning optimization technique like FedProx or FedAvgM to improve convergence speed or model quality.